{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Github_Full ANN Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ8ElxAElgyx"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml8dgZJlCFAk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "7c11a811-c4f6-4bd1-d5b8-f729c6f5e90d"
      },
      "source": [
        "df = pd.read_csv('NYCTaxiFares.csv')\n",
        "df #here we're try to predict the fare_class column (0 if it's <$10, 1 if it's >$10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>fare_class</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010-04-19 08:17:56 UTC</td>\n",
              "      <td>6.5</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.992365</td>\n",
              "      <td>40.730521</td>\n",
              "      <td>-73.975499</td>\n",
              "      <td>40.744746</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-04-17 15:43:53 UTC</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.990078</td>\n",
              "      <td>40.740558</td>\n",
              "      <td>-73.974232</td>\n",
              "      <td>40.744114</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2010-04-17 11:23:26 UTC</td>\n",
              "      <td>10.1</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.994149</td>\n",
              "      <td>40.751118</td>\n",
              "      <td>-73.960064</td>\n",
              "      <td>40.766235</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2010-04-11 21:25:03 UTC</td>\n",
              "      <td>8.9</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.990485</td>\n",
              "      <td>40.756422</td>\n",
              "      <td>-73.971205</td>\n",
              "      <td>40.748192</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-04-17 02:19:01 UTC</td>\n",
              "      <td>19.7</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.990976</td>\n",
              "      <td>40.734202</td>\n",
              "      <td>-73.905956</td>\n",
              "      <td>40.743115</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119995</th>\n",
              "      <td>2010-04-18 14:33:03 UTC</td>\n",
              "      <td>15.3</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.955857</td>\n",
              "      <td>40.784590</td>\n",
              "      <td>-73.981941</td>\n",
              "      <td>40.736789</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119996</th>\n",
              "      <td>2010-04-23 10:27:48 UTC</td>\n",
              "      <td>15.3</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.996329</td>\n",
              "      <td>40.772727</td>\n",
              "      <td>-74.049890</td>\n",
              "      <td>40.740413</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119997</th>\n",
              "      <td>2010-04-18 18:50:40 UTC</td>\n",
              "      <td>12.5</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.988574</td>\n",
              "      <td>40.749772</td>\n",
              "      <td>-74.011541</td>\n",
              "      <td>40.707799</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119998</th>\n",
              "      <td>2010-04-13 08:14:44 UTC</td>\n",
              "      <td>4.9</td>\n",
              "      <td>0</td>\n",
              "      <td>-74.004449</td>\n",
              "      <td>40.724529</td>\n",
              "      <td>-73.992697</td>\n",
              "      <td>40.730765</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119999</th>\n",
              "      <td>2010-04-17 16:00:14 UTC</td>\n",
              "      <td>5.3</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.955415</td>\n",
              "      <td>40.771920</td>\n",
              "      <td>-73.967623</td>\n",
              "      <td>40.763015</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120000 rows Ã— 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                pickup_datetime  fare_amount  ...  dropoff_latitude  passenger_count\n",
              "0       2010-04-19 08:17:56 UTC          6.5  ...         40.744746                1\n",
              "1       2010-04-17 15:43:53 UTC          6.9  ...         40.744114                1\n",
              "2       2010-04-17 11:23:26 UTC         10.1  ...         40.766235                2\n",
              "3       2010-04-11 21:25:03 UTC          8.9  ...         40.748192                1\n",
              "4       2010-04-17 02:19:01 UTC         19.7  ...         40.743115                1\n",
              "...                         ...          ...  ...               ...              ...\n",
              "119995  2010-04-18 14:33:03 UTC         15.3  ...         40.736789                1\n",
              "119996  2010-04-23 10:27:48 UTC         15.3  ...         40.740413                1\n",
              "119997  2010-04-18 18:50:40 UTC         12.5  ...         40.707799                3\n",
              "119998  2010-04-13 08:14:44 UTC          4.9  ...         40.730765                1\n",
              "119999  2010-04-17 16:00:14 UTC          5.3  ...         40.763015                3\n",
              "\n",
              "[120000 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcZzUsp_eve8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b03bc7d-2853-4ce4-c555-5329cd2ff70e"
      },
      "source": [
        "df['fare_class'].value_counts() #80000 cases of class 0 and 40000 cases of class 1"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    80000\n",
              "1    40000\n",
              "Name: fare_class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEIuCChUGPH7"
      },
      "source": [
        "# **Part 1**\n",
        "Feature Engineering 1: Distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Opo0ikUlDBf_"
      },
      "source": [
        "#Below is the Haversine distance formula to calculate the distance travelled on a sphere. So in our case, we'd be using it to calculate distance between the pick up/drop off longitudes/latitudes on Earth\n",
        "#https://en.wikipedia.org/wiki/Haversine_formula\n",
        "def Haversine(dataframe, pickup_lat, pickup_long, dropoff_lat, dropoff_long):\n",
        "    r = 6371  #average radius in km of Earth\n",
        "       \n",
        "    phi1 = np.radians(df[pickup_lat]) #converting into radians\n",
        "    phi2 = np.radians(df[dropoff_lat]) #converting into radians\n",
        "    \n",
        "    delta_phi = np.radians(df[dropoff_lat]-df[pickup_lat]) #converting into radians\n",
        "    delta_lambda = np.radians(df[dropoff_long]-df[pickup_long]) #converting into radians\n",
        "     \n",
        "    a = np.sin(delta_phi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda/2)**2 #Haversine distance formula\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
        "    d = (r * c) # in kilometers\n",
        "\n",
        "    return d"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRz-9k0zEfh-"
      },
      "source": [
        "df['dist_km'] = Haversine(df, 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude') #adding a new column to contain the distance calculated by the haversine function"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5opgPT3Evd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "4ad79f16-5167-47d3-c82e-f06259793298"
      },
      "source": [
        "#We just performed Feature Engineering\n",
        "#Feature Engineering is taking the features we are already given and creating a new one that will be more useful for our model. In this case, we feature engineered the dist_km column using our latitude and longitude data\n",
        "#The reason we created this dist_km column is because if we tried creating our model that took in just the longitude/latitude, that's a lot more data to process and the extra variables may result in poor results. In addition, the longitude/latitude data points are not too big of a difference from each other. As such, creating a new, distinct single feature (dist_km) could result in faster computation and better results.\n",
        "df.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>fare_class</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>dist_km</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010-04-19 08:17:56 UTC</td>\n",
              "      <td>6.5</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.992365</td>\n",
              "      <td>40.730521</td>\n",
              "      <td>-73.975499</td>\n",
              "      <td>40.744746</td>\n",
              "      <td>1</td>\n",
              "      <td>2.126312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-04-17 15:43:53 UTC</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.990078</td>\n",
              "      <td>40.740558</td>\n",
              "      <td>-73.974232</td>\n",
              "      <td>40.744114</td>\n",
              "      <td>1</td>\n",
              "      <td>1.392307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2010-04-17 11:23:26 UTC</td>\n",
              "      <td>10.1</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.994149</td>\n",
              "      <td>40.751118</td>\n",
              "      <td>-73.960064</td>\n",
              "      <td>40.766235</td>\n",
              "      <td>2</td>\n",
              "      <td>3.326763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2010-04-11 21:25:03 UTC</td>\n",
              "      <td>8.9</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.990485</td>\n",
              "      <td>40.756422</td>\n",
              "      <td>-73.971205</td>\n",
              "      <td>40.748192</td>\n",
              "      <td>1</td>\n",
              "      <td>1.864129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-04-17 02:19:01 UTC</td>\n",
              "      <td>19.7</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.990976</td>\n",
              "      <td>40.734202</td>\n",
              "      <td>-73.905956</td>\n",
              "      <td>40.743115</td>\n",
              "      <td>1</td>\n",
              "      <td>7.231321</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           pickup_datetime  fare_amount  ...  passenger_count   dist_km\n",
              "0  2010-04-19 08:17:56 UTC          6.5  ...                1  2.126312\n",
              "1  2010-04-17 15:43:53 UTC          6.9  ...                1  1.392307\n",
              "2  2010-04-17 11:23:26 UTC         10.1  ...                2  3.326763\n",
              "3  2010-04-11 21:25:03 UTC          8.9  ...                1  1.864129\n",
              "4  2010-04-17 02:19:01 UTC         19.7  ...                1  7.231321\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjbfz-dIGSRz"
      },
      "source": [
        "Feature Engineering 2: Date-Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-RFOoCpDr1V",
        "outputId": "5d04e91a-8679-437f-e214-856cd001285f"
      },
      "source": [
        "df.info() #notice how pickup_datetime has Dtype = object, meaning it's treated like a string right now"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 120000 entries, 0 to 119999\n",
            "Data columns (total 9 columns):\n",
            " #   Column             Non-Null Count   Dtype  \n",
            "---  ------             --------------   -----  \n",
            " 0   pickup_datetime    120000 non-null  object \n",
            " 1   fare_amount        120000 non-null  float64\n",
            " 2   fare_class         120000 non-null  int64  \n",
            " 3   pickup_longitude   120000 non-null  float64\n",
            " 4   pickup_latitude    120000 non-null  float64\n",
            " 5   dropoff_longitude  120000 non-null  float64\n",
            " 6   dropoff_latitude   120000 non-null  float64\n",
            " 7   passenger_count    120000 non-null  int64  \n",
            " 8   dist_km            120000 non-null  float64\n",
            "dtypes: float64(6), int64(2), object(1)\n",
            "memory usage: 8.2+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5df7eBiLGaWe"
      },
      "source": [
        "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime']) #converting our pickup_datetime column into an actual date time rather than a string"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpYopKgcG1DI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acc6a469-71d8-4d0c-faef-f684d75a16a7"
      },
      "source": [
        "df.info() #notice how it's a datetime object now"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 120000 entries, 0 to 119999\n",
            "Data columns (total 9 columns):\n",
            " #   Column             Non-Null Count   Dtype              \n",
            "---  ------             --------------   -----              \n",
            " 0   pickup_datetime    120000 non-null  datetime64[ns, UTC]\n",
            " 1   fare_amount        120000 non-null  float64            \n",
            " 2   fare_class         120000 non-null  int64              \n",
            " 3   pickup_longitude   120000 non-null  float64            \n",
            " 4   pickup_latitude    120000 non-null  float64            \n",
            " 5   dropoff_longitude  120000 non-null  float64            \n",
            " 6   dropoff_latitude   120000 non-null  float64            \n",
            " 7   passenger_count    120000 non-null  int64              \n",
            " 8   dist_km            120000 non-null  float64            \n",
            "dtypes: datetime64[ns, UTC](1), float64(6), int64(2)\n",
            "memory usage: 8.2 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyKrhd0pHoGZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "cacdc4ab-85ab-4c87-bdea-20c8602d8a2b"
      },
      "source": [
        "df['EDTdate'] = df['pickup_datetime'] - pd.Timedelta(hours = 4) #creating a new column to have our date time from UTC to EDT\n",
        "df.head()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>fare_class</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>dist_km</th>\n",
              "      <th>EDTdate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010-04-19 08:17:56+00:00</td>\n",
              "      <td>6.5</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.992365</td>\n",
              "      <td>40.730521</td>\n",
              "      <td>-73.975499</td>\n",
              "      <td>40.744746</td>\n",
              "      <td>1</td>\n",
              "      <td>2.126312</td>\n",
              "      <td>2010-04-19 04:17:56+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-04-17 15:43:53+00:00</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.990078</td>\n",
              "      <td>40.740558</td>\n",
              "      <td>-73.974232</td>\n",
              "      <td>40.744114</td>\n",
              "      <td>1</td>\n",
              "      <td>1.392307</td>\n",
              "      <td>2010-04-17 11:43:53+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2010-04-17 11:23:26+00:00</td>\n",
              "      <td>10.1</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.994149</td>\n",
              "      <td>40.751118</td>\n",
              "      <td>-73.960064</td>\n",
              "      <td>40.766235</td>\n",
              "      <td>2</td>\n",
              "      <td>3.326763</td>\n",
              "      <td>2010-04-17 07:23:26+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2010-04-11 21:25:03+00:00</td>\n",
              "      <td>8.9</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.990485</td>\n",
              "      <td>40.756422</td>\n",
              "      <td>-73.971205</td>\n",
              "      <td>40.748192</td>\n",
              "      <td>1</td>\n",
              "      <td>1.864129</td>\n",
              "      <td>2010-04-11 17:25:03+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-04-17 02:19:01+00:00</td>\n",
              "      <td>19.7</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.990976</td>\n",
              "      <td>40.734202</td>\n",
              "      <td>-73.905956</td>\n",
              "      <td>40.743115</td>\n",
              "      <td>1</td>\n",
              "      <td>7.231321</td>\n",
              "      <td>2010-04-16 22:19:01+00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            pickup_datetime  fare_amount  ...   dist_km                   EDTdate\n",
              "0 2010-04-19 08:17:56+00:00          6.5  ...  2.126312 2010-04-19 04:17:56+00:00\n",
              "1 2010-04-17 15:43:53+00:00          6.9  ...  1.392307 2010-04-17 11:43:53+00:00\n",
              "2 2010-04-17 11:23:26+00:00         10.1  ...  3.326763 2010-04-17 07:23:26+00:00\n",
              "3 2010-04-11 21:25:03+00:00          8.9  ...  1.864129 2010-04-11 17:25:03+00:00\n",
              "4 2010-04-17 02:19:01+00:00         19.7  ...  7.231321 2010-04-16 22:19:01+00:00\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEzWpO_4Ift9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "189b4dba-afe0-4b48-db80-ba40bb2c545c"
      },
      "source": [
        "#so a neural network isn't going to understand what a date time object is when being fed into it. So we need to perform feature engineering to grab specific information from our date time object that can be fed into our NN.\n",
        "#Now that we're converted our pickup_datetime into a datetime object, it can be broken up into components, such as hour, minute, day, etc\n",
        "df['Hour'] = df['EDTdate'].dt.hour #making a new column to get the hour\n",
        "df['AMorPM'] = np.where( df['Hour'] < 12, 'am', 'pm') #if hour is less than 12, then it's AM, else it's PM\n",
        "df['Weekday'] = df['EDTdate'].dt.strftime(\"%a\") #getting the date of the week. Can also use .dt.dayofweek\n",
        "df.head() #So we feature engineered 3 more columns with specific information about the date time"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>fare_class</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>dist_km</th>\n",
              "      <th>EDTdate</th>\n",
              "      <th>Hour</th>\n",
              "      <th>AMorPM</th>\n",
              "      <th>Weekday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010-04-19 08:17:56+00:00</td>\n",
              "      <td>6.5</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.992365</td>\n",
              "      <td>40.730521</td>\n",
              "      <td>-73.975499</td>\n",
              "      <td>40.744746</td>\n",
              "      <td>1</td>\n",
              "      <td>2.126312</td>\n",
              "      <td>2010-04-19 04:17:56+00:00</td>\n",
              "      <td>4</td>\n",
              "      <td>am</td>\n",
              "      <td>Mon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-04-17 15:43:53+00:00</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.990078</td>\n",
              "      <td>40.740558</td>\n",
              "      <td>-73.974232</td>\n",
              "      <td>40.744114</td>\n",
              "      <td>1</td>\n",
              "      <td>1.392307</td>\n",
              "      <td>2010-04-17 11:43:53+00:00</td>\n",
              "      <td>11</td>\n",
              "      <td>am</td>\n",
              "      <td>Sat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2010-04-17 11:23:26+00:00</td>\n",
              "      <td>10.1</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.994149</td>\n",
              "      <td>40.751118</td>\n",
              "      <td>-73.960064</td>\n",
              "      <td>40.766235</td>\n",
              "      <td>2</td>\n",
              "      <td>3.326763</td>\n",
              "      <td>2010-04-17 07:23:26+00:00</td>\n",
              "      <td>7</td>\n",
              "      <td>am</td>\n",
              "      <td>Sat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2010-04-11 21:25:03+00:00</td>\n",
              "      <td>8.9</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.990485</td>\n",
              "      <td>40.756422</td>\n",
              "      <td>-73.971205</td>\n",
              "      <td>40.748192</td>\n",
              "      <td>1</td>\n",
              "      <td>1.864129</td>\n",
              "      <td>2010-04-11 17:25:03+00:00</td>\n",
              "      <td>17</td>\n",
              "      <td>pm</td>\n",
              "      <td>Sun</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-04-17 02:19:01+00:00</td>\n",
              "      <td>19.7</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.990976</td>\n",
              "      <td>40.734202</td>\n",
              "      <td>-73.905956</td>\n",
              "      <td>40.743115</td>\n",
              "      <td>1</td>\n",
              "      <td>7.231321</td>\n",
              "      <td>2010-04-16 22:19:01+00:00</td>\n",
              "      <td>22</td>\n",
              "      <td>pm</td>\n",
              "      <td>Fri</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            pickup_datetime  fare_amount  fare_class  ...  Hour  AMorPM  Weekday\n",
              "0 2010-04-19 08:17:56+00:00          6.5           0  ...     4      am      Mon\n",
              "1 2010-04-17 15:43:53+00:00          6.9           0  ...    11      am      Sat\n",
              "2 2010-04-17 11:23:26+00:00         10.1           1  ...     7      am      Sat\n",
              "3 2010-04-11 21:25:03+00:00          8.9           0  ...    17      pm      Sun\n",
              "4 2010-04-17 02:19:01+00:00         19.7           1  ...    22      pm      Fri\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUHol08LK4jc"
      },
      "source": [
        "# **Part 2**\n",
        "Separating Categorical Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LMYlT74J_Fp"
      },
      "source": [
        "categorical_col = ['Hour', 'AMorPM', 'Weekday'] #categorical columns\n",
        "continuous_col = ['pickup_longitude','pickup_latitude', 'dropoff_longitude', 'dropoff_latitude','passenger_count', 'dist_km'] #continuous columns\n",
        "y_col = ['fare_class'] #this will be the answer that we want to be the output of our NN"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_3djIz3Lo5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4679b1c6-ac3a-4c34-ea3a-aa14970d62b8"
      },
      "source": [
        "df.dtypes #notice weekday and AMorPM are objects, but we want to convert them into categoires"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pickup_datetime      datetime64[ns, UTC]\n",
              "fare_amount                      float64\n",
              "fare_class                         int64\n",
              "pickup_longitude                 float64\n",
              "pickup_latitude                  float64\n",
              "dropoff_longitude                float64\n",
              "dropoff_latitude                 float64\n",
              "passenger_count                    int64\n",
              "dist_km                          float64\n",
              "EDTdate              datetime64[ns, UTC]\n",
              "Hour                               int64\n",
              "AMorPM                            object\n",
              "Weekday                           object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJAmwJt3MM1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf38edd-ed34-4544-f6fa-589d956721d4"
      },
      "source": [
        "for cat in categorical_col:\n",
        "  #looping through the categorical_col list so we can convert each column listed into a category datatype\n",
        "  df[cat] = df[cat].astype('category')\n",
        "\n",
        "df.dtypes #notice they are categories now"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pickup_datetime      datetime64[ns, UTC]\n",
              "fare_amount                      float64\n",
              "fare_class                         int64\n",
              "pickup_longitude                 float64\n",
              "pickup_latitude                  float64\n",
              "dropoff_longitude                float64\n",
              "dropoff_latitude                 float64\n",
              "passenger_count                    int64\n",
              "dist_km                          float64\n",
              "EDTdate              datetime64[ns, UTC]\n",
              "Hour                            category\n",
              "AMorPM                          category\n",
              "Weekday                         category\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtjtLBwrMm61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e12bee8-9562-4c05-faae-c7a76df137fb"
      },
      "source": [
        "df['Weekday'].head() #notice the weekday column is has 7 categories\r\n",
        "#similarly, AMorPM would have 2 categories, Hour would have 24 categories, etc"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Mon\n",
              "1    Sat\n",
              "2    Sat\n",
              "3    Sun\n",
              "4    Fri\n",
              "Name: Weekday, dtype: category\n",
              "Categories (7, object): ['Fri', 'Mon', 'Sat', 'Sun', 'Thu', 'Tue', 'Wed']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gny98utYM59_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dfa99ba-c8f0-4517-88b6-6811d102a520"
      },
      "source": [
        "print(df['Weekday'].cat.codes.values) #so we're given a code for each day of the weekend from 0 to 6\r\n",
        "print(df['AMorPM'].cat.codes.values) #so we're given a code for AM = 0, PM = 1"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 2 ... 3 5 2]\n",
            "[0 0 0 ... 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChU3widDN5rM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed100c11-d023-4638-afb2-124a66038e67"
      },
      "source": [
        "hr = df['Hour'].cat.codes.values #extracting the code for each hour\n",
        "ampm = df['AMorPM'].cat.codes.values  #extracting the code for AM and PM\n",
        "wkdy = df['Weekday'].cat.codes.values  #extracting the code for each day of the week\n",
        "\n",
        "cats = np.stack([hr, ampm, wkdy], axis = 1) #so we're stacking the arrays together along the y-axis to make a row for each\n",
        "cats #first entry is 4 AM on Monday\n",
        "\n",
        "#could've done this all in one line using:\n",
        "#cats = np.stack([df[col].cat.codes.values for col in categorical_col], 1)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4,  0,  1],\n",
              "       [11,  0,  2],\n",
              "       [ 7,  0,  2],\n",
              "       ...,\n",
              "       [14,  1,  3],\n",
              "       [ 4,  0,  5],\n",
              "       [12,  1,  2]], dtype=int8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8ziNMExONQn"
      },
      "source": [
        "cats = torch.tensor(cats, dtype = torch.int64) #converting our categories into a PyTorch Tensor to feed into our model"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agjt9BIIPgOH"
      },
      "source": [
        "Separating Continuous Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcZf6744Pfe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a808f11-3d5e-4622-d407-52052596999c"
      },
      "source": [
        "conts = np.stack([df[col].values for col in continuous_col], axis = 1) #doing what we did above to categorical values, except in one line for continuous\n",
        "conts "
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-73.992365  ,  40.730521  , -73.975499  ,  40.744746  ,\n",
              "          1.        ,   2.12631159],\n",
              "       [-73.990078  ,  40.740558  , -73.974232  ,  40.744114  ,\n",
              "          1.        ,   1.39230687],\n",
              "       [-73.994149  ,  40.751118  , -73.960064  ,  40.766235  ,\n",
              "          2.        ,   3.32676344],\n",
              "       ...,\n",
              "       [-73.988574  ,  40.749772  , -74.011541  ,  40.707799  ,\n",
              "          3.        ,   5.05252282],\n",
              "       [-74.004449  ,  40.724529  , -73.992697  ,  40.730765  ,\n",
              "          1.        ,   1.20892296],\n",
              "       [-73.955415  ,  40.77192   , -73.967623  ,  40.763015  ,\n",
              "          3.        ,   1.42739869]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7Adc9GJQK7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e87849da-3b19-4635-a104-c7aaf8333e5d"
      },
      "source": [
        "conts = torch.tensor(conts, dtype=torch.float) #converting our continous variables into a PyTorch Tensor feed into our model\n",
        "conts"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-73.9924,  40.7305, -73.9755,  40.7447,   1.0000,   2.1263],\n",
              "        [-73.9901,  40.7406, -73.9742,  40.7441,   1.0000,   1.3923],\n",
              "        [-73.9941,  40.7511, -73.9601,  40.7662,   2.0000,   3.3268],\n",
              "        ...,\n",
              "        [-73.9886,  40.7498, -74.0115,  40.7078,   3.0000,   5.0525],\n",
              "        [-74.0044,  40.7245, -73.9927,  40.7308,   1.0000,   1.2089],\n",
              "        [-73.9554,  40.7719, -73.9676,  40.7630,   3.0000,   1.4274]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiFQjEH3QV3a"
      },
      "source": [
        "Separating The Label (Answer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_08vX-XQT6X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a16475f2-cf3e-41af-919b-f12d948e6501"
      },
      "source": [
        "y = torch.tensor(df[y_col].values).flatten() \n",
        "y"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 1,  ..., 1, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn5T1IP4Qomu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fdd8072-c815-47ae-a25c-5a3be5747488"
      },
      "source": [
        "print('cats.shape =',cats.shape)\n",
        "print('conts.shape =',conts.shape)\n",
        "print('y.shape =',y.shape)\n",
        "#shapes are as expected with 3 columns in categorical, 6 columns in continuous, and 1 column as the label"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cats.shape = torch.Size([120000, 3])\n",
            "conts.shape = torch.Size([120000, 6])\n",
            "y.shape = torch.Size([120000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVuTBWmTRqxv"
      },
      "source": [
        "Embedding our Categories (One Hot Encoding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDD58WB-RIMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14188228-4beb-44cf-917c-49f100306b45"
      },
      "source": [
        "categorical_sizes = [len(df[col].cat.categories) for col in categorical_col] #category sizes\n",
        "\n",
        "categorical_sizes #shows how many categories we have in each column, so our case is 24 hours in a day, 2 (AM or PM), and 7 days in a week"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[24, 2, 7]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wyta0DW6SOKS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "365c1b28-574f-4a87-d0cf-93102c81d90a"
      },
      "source": [
        "emb_sizes = [(size, min(50, (size+1)//2)) for size in categorical_sizes] #so for size (24 ,2 ,7) in categorical_sizes, embed 50 or take the size and divide by 2\n",
        "#the +1 is incase we only have 2 categories and // is to make sure the output is an integer\n",
        "emb_sizes #none of the sizes exceeded 50, so it just made the embedding size the size divided by 2. \n",
        "#so here, our number of categories are 24, 2, 7 and our embedding sizes are 12, 1, 4\n",
        "\n",
        "#So the embedding provides a more compact (lower dimensional) representation of a set of input variables that have some correlations. It's useful because it reduces the number of parameters overall. \n",
        "#So technically we could've just used all 24 hours + 2 am or pm + 7 days of the week as inputs into our NN, but we used embedding to reduce the parameters and make calculations faster\n",
        "#An embedding layer simply maps/projects some N dimensional tensor into another number of dimensions, the number of dimensions to output to are often arbitrary, but you can have some logic to it, like embedding 7 days a week to 2 dimensions may end up mapping closely to weekday vs weekends (although there is no guarantee of this)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(24, 12), (2, 1), (7, 4)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wSwBB0jC6RC"
      },
      "source": [
        "# **Part 3**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a27YDHSlKmXZ"
      },
      "source": [
        "Creating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldYsdyJEJAHX"
      },
      "source": [
        "#Inheriting from nn.Module to use it's features\n",
        "class Model(nn.Module):\n",
        "  \n",
        "  def __init__(self, emb_sizes, n_continuous, out_size, n_layers, p=0.5):\n",
        "    #so our parameters are embedding size, number of continous features, output size, number of layers, and our dropout probability defaulted to 50%\n",
        "    #This determins how many layers are there in our NN\n",
        "\n",
        "    super().__init__() #inheriting the features from nn.Module\n",
        "    self.embeds = nn.ModuleList([nn.Embedding(num_embeddings = ni, embedding_dim = nf) for ni,nf in emb_sizes]) #so now our ModuleList is filled with the Embedding layers (one for hour, one of ampm, and one for weekday)\n",
        "    self.emb_drop = nn.Dropout(p) #adding a dropout layer\n",
        "    self.bn_cont = nn.BatchNorm1d(n_continuous) #normalizing the continous data so it falls within the same order of magnitude range\n",
        "\n",
        "    layerlist = [] #initializing layerlist so we can store our layers\n",
        "    n_emb = sum([nf for ni,nf in emb_sizes]) #calculating the number of embeddings, which is basically 12 + 1 + 4\n",
        "    n_in = n_emb + n_continuous #so the total number of inputs will be the number of embeddings plus the number of continous features\n",
        "\n",
        "    for i in n_layers:\n",
        "      layerlist.append(nn.Linear(n_in, i)) #create a layer connecting n_in and i, where i is the number of neurons. Basically this makes a fully connected layer. \n",
        "      layerlist.append(nn.ReLU(inplace = True)) #adding the activation function to the layer\n",
        "      layerlist.append(nn.BatchNorm1d(i)) #adding the normalization of the continuous values to the list\n",
        "      layerlist.append(nn.Dropout(p)) #adding the dropout to the layer\n",
        "      n_in = i #equating n_in with the number of neurons i, so that the next fully connected layer can start with i - 1 neurons.\n",
        "\n",
        "    #So layers is going to look like this: layers = [256, 128, 64] meaning you want 256 neurons in first layer, then 128 neurons in second layer, then 64. This allows us play around with the values and makes it flexible. \n",
        "    layerlist.append(nn.Linear(n_layers[-1], out_size)) #so making the last layer be connected by the last layer and output size\n",
        "    self.layers = nn.Sequential(*layerlist) #using sequential to combine the layers from the layerlist so that it can be ordered properly as a NN\n",
        "\n",
        "  def forward(self, x_categorical, x_continuous):\n",
        "    #So this determines the forward propagation with the activation functions we want to use\n",
        "\n",
        "    # Now we are utilizing the embeddings to pass into our forward method\n",
        "    embeddings = [] \n",
        "\n",
        "    for i,e in enumerate(self.embeds):\n",
        "      embeddings.append(e(x_categorical[:,i])) #so we're adding to our embeddingz list for each row and column i in our categorical inputs, x_cat\n",
        "\n",
        "    x = torch.cat(tensors = embeddings, dim = 1) #so we're concatenating embeddings along dimension 1, meaning all the values we got above for each tensor will be within a single row (i.e. 12(hour) + 1(amorpm) + 4(weekday) all in one row within a single tensor)\n",
        "    x = self.emb_drop(x) #adding the drop out layer to our embedded categorical data\n",
        "\n",
        "    x_cont = self.bn_cont(x_continuous) #normalizing our continuous data\n",
        "    x = torch.cat(tensors = [x, x_continuous], dim = 1) #concatenating our categorical and continuous data to be in a single tensor row\n",
        "    x = self.layers(x) #passing our continuous and categorical data into our NN, which was made above with Sequential\n",
        "\n",
        "    return x"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUlOYxetCvRX"
      },
      "source": [
        "# **Part 4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJJbQ5KhS1WJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1974de14-a4fd-47f1-af1d-a29f49d27850"
      },
      "source": [
        "model = Model(emb_sizes,  n_continuous = conts.shape[1], out_size = 2, n_layers = [256, 128, 64]) #conts.shape[1] is the number of columns in the continuous tensor, output size is 2 (since we have 2 classes, 0 or 1, which is a binary classification problem), and hidden layers will have 256 then 128 then 64 neurons\n",
        "model"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (embeds): ModuleList(\n",
              "    (0): Embedding(24, 12)\n",
              "    (1): Embedding(2, 1)\n",
              "    (2): Embedding(7, 4)\n",
              "  )\n",
              "  (emb_drop): Dropout(p=0.5, inplace=False)\n",
              "  (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=23, out_features=256, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): Dropout(p=0.5, inplace=False)\n",
              "    (8): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): Dropout(p=0.5, inplace=False)\n",
              "    (12): Linear(in_features=64, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJhFUJ3fDk-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be9014e0-47cf-4033-c712-3dbb72be347d"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss() #so our loss measurement will be based off CrossEntropyLoss since this is a mutually exclusive answer (only 1 target can be correct)\n",
        "\n",
        "optimizer = torch.optim.Adam(params = model.parameters(), lr = 0.001) #model parameters are just the fully connected layers and we are using Adam optimizer to optimize them\n",
        "model.parameters #can see the parameters are just the fully connected layers and we are optimizing them"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of Model(\n",
              "  (embeds): ModuleList(\n",
              "    (0): Embedding(24, 12)\n",
              "    (1): Embedding(2, 1)\n",
              "    (2): Embedding(7, 4)\n",
              "  )\n",
              "  (emb_drop): Dropout(p=0.5, inplace=False)\n",
              "  (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=23, out_features=256, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): Dropout(p=0.5, inplace=False)\n",
              "    (8): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): Dropout(p=0.5, inplace=False)\n",
              "    (12): Linear(in_features=64, out_features=2, bias=True)\n",
              "  )\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lBVqiF_Et2E"
      },
      "source": [
        "batch_size = 120000 #just going to feed in all the data\n",
        "test_size = int(batch_size*0.25) #making our test size 1/3 the batch size"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1j5OP9mFFt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d429976-4f3c-400c-b810-d626148c4a1e"
      },
      "source": [
        "#Our data was shuffled already when we got it, which is why we can do the following. If it wasn't shuffled already, then we'd need to shuffle first before assigning the train test split\n",
        "\n",
        "categorical_train = cats[:batch_size - test_size] #so take from index 0 to 24000\n",
        "categorical_test = cats[batch_size - test_size: batch_size] #take from index 24000 to 30000\n",
        "continuous_train = conts[:batch_size - test_size]\n",
        "continuous_test = conts[batch_size - test_size: batch_size]\n",
        "y_train = y[:batch_size - test_size]\n",
        "y_test = y[batch_size - test_size: batch_size]\n",
        "\n",
        "print(len(categorical_train))\n",
        "print(len(categorical_test))\n",
        "print(len(continuous_train))\n",
        "print(len(continuous_test))\n",
        "print(len(y_train))\n",
        "print(len(y_test))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "90000\n",
            "30000\n",
            "90000\n",
            "30000\n",
            "90000\n",
            "30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6SCyakqHI8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec68ebe2-4b6e-4dd2-c3de-9bf6af67532e"
      },
      "source": [
        "import time #Gonna try to keep track of how long it takes to train our model\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "epochs = 450\n",
        "\n",
        "losses = []\n",
        "\n",
        "#This for loop trains our NN\n",
        "for i in range(epochs):\n",
        "  #Forward propagation through our ANN using training data\n",
        "  y_pred = model.forward(categorical_train, continuous_train)\n",
        "\n",
        "  #Calculating loss/error\n",
        "  loss = criterion(y_pred, y_train)\n",
        "  losses.append(loss)\n",
        "\n",
        "  print(f'epoch {i} and loss is: {loss} and time: {(time.time() - start_time)/60}')\n",
        "\n",
        "  #Backpropagation\n",
        "  optimizer.zero_grad() #resetting the gradient on the optimizer so it doesn't accumulate\n",
        "  loss.backward() #doing backpropagation off the loss function\n",
        "  optimizer.step() #using the optimizer for the back propagation\n",
        "\n",
        "print(f'Training took {(time.time() - start_time)/60} minutes')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 and loss is: 0.7902092933654785 and time: 0.054344324270884196\n",
            "epoch 1 and loss is: 0.6657267808914185 and time: 0.17891168594360352\n",
            "epoch 2 and loss is: 0.5839962363243103 and time: 0.2994275689125061\n",
            "epoch 3 and loss is: 0.5252201557159424 and time: 0.4179833014806112\n",
            "epoch 4 and loss is: 0.48761048913002014 and time: 0.5386810501416525\n",
            "epoch 5 and loss is: 0.45757704973220825 and time: 0.6567998210589091\n",
            "epoch 6 and loss is: 0.43692222237586975 and time: 0.7758350292841594\n",
            "epoch 7 and loss is: 0.41791242361068726 and time: 0.8950976371765137\n",
            "epoch 8 and loss is: 0.4041275680065155 and time: 1.013663681348165\n",
            "epoch 9 and loss is: 0.3927142322063446 and time: 1.1297000726064046\n",
            "epoch 10 and loss is: 0.38150912523269653 and time: 1.2482161323229473\n",
            "epoch 11 and loss is: 0.37333202362060547 and time: 1.3689636667569478\n",
            "epoch 12 and loss is: 0.36476805806159973 and time: 1.486493984858195\n",
            "epoch 13 and loss is: 0.3576088845729828 and time: 1.604097851117452\n",
            "epoch 14 and loss is: 0.35224011540412903 and time: 1.7221288482348125\n",
            "epoch 15 and loss is: 0.34805548191070557 and time: 1.8422153830528258\n",
            "epoch 16 and loss is: 0.3419075012207031 and time: 1.9605772693951924\n",
            "epoch 17 and loss is: 0.3372882306575775 and time: 2.0778847376505536\n",
            "epoch 18 and loss is: 0.3341880440711975 and time: 2.19473405679067\n",
            "epoch 19 and loss is: 0.3301433324813843 and time: 2.3112530946731566\n",
            "epoch 20 and loss is: 0.32863858342170715 and time: 2.4297018170356752\n",
            "epoch 21 and loss is: 0.3242702782154083 and time: 2.547158193588257\n",
            "epoch 22 and loss is: 0.3237670660018921 and time: 2.6634172717730205\n",
            "epoch 23 and loss is: 0.32201579213142395 and time: 2.7787705381711323\n",
            "epoch 24 and loss is: 0.32108545303344727 and time: 2.8958542863527934\n",
            "epoch 25 and loss is: 0.3181300461292267 and time: 3.0119704564412433\n",
            "epoch 26 and loss is: 0.31709375977516174 and time: 3.131100074450175\n",
            "epoch 27 and loss is: 0.318593829870224 and time: 3.2515425006548564\n",
            "epoch 28 and loss is: 0.31570789217948914 and time: 3.37081245581309\n",
            "epoch 29 and loss is: 0.31599998474121094 and time: 3.4895310163497926\n",
            "epoch 30 and loss is: 0.314591646194458 and time: 3.6061575770378114\n",
            "epoch 31 and loss is: 0.31228986382484436 and time: 3.725042100747426\n",
            "epoch 32 and loss is: 0.31382089853286743 and time: 3.844759241739909\n",
            "epoch 33 and loss is: 0.31204909086227417 and time: 3.961055600643158\n",
            "epoch 34 and loss is: 0.31069907546043396 and time: 4.079883150259653\n",
            "epoch 35 and loss is: 0.31035754084587097 and time: 4.199522542953491\n",
            "epoch 36 and loss is: 0.3101683259010315 and time: 4.317514085769654\n",
            "epoch 37 and loss is: 0.3105399012565613 and time: 4.436267813046773\n",
            "epoch 38 and loss is: 0.3089059889316559 and time: 4.554418158531189\n",
            "epoch 39 and loss is: 0.30945929884910583 and time: 4.673369781176249\n",
            "epoch 40 and loss is: 0.3078919053077698 and time: 4.790676935513814\n",
            "epoch 41 and loss is: 0.30751359462738037 and time: 4.907708021004995\n",
            "epoch 42 and loss is: 0.307496577501297 and time: 5.02405682404836\n",
            "epoch 43 and loss is: 0.3069522976875305 and time: 5.141887080669403\n",
            "epoch 44 and loss is: 0.3071131408214569 and time: 5.2601197242736815\n",
            "epoch 45 and loss is: 0.30449679493904114 and time: 5.380119454860687\n",
            "epoch 46 and loss is: 0.3039824664592743 and time: 5.50004673798879\n",
            "epoch 47 and loss is: 0.3043338656425476 and time: 5.620314280192058\n",
            "epoch 48 and loss is: 0.3044029772281647 and time: 5.740560972690583\n",
            "epoch 49 and loss is: 0.3036891520023346 and time: 5.858648693561554\n",
            "epoch 50 and loss is: 0.30534738302230835 and time: 5.9748436888058984\n",
            "epoch 51 and loss is: 0.30456364154815674 and time: 6.092652213573456\n",
            "epoch 52 and loss is: 0.3045947253704071 and time: 6.207259420553843\n",
            "epoch 53 and loss is: 0.3042687177658081 and time: 6.323577793439229\n",
            "epoch 54 and loss is: 0.30226650834083557 and time: 6.4412238279978435\n",
            "epoch 55 and loss is: 0.30395954847335815 and time: 6.558801050980886\n",
            "epoch 56 and loss is: 0.301237553358078 and time: 6.676890861988068\n",
            "epoch 57 and loss is: 0.30219927430152893 and time: 6.794474673271179\n",
            "epoch 58 and loss is: 0.3007354140281677 and time: 6.910874990622203\n",
            "epoch 59 and loss is: 0.30066725611686707 and time: 7.031127448876699\n",
            "epoch 60 and loss is: 0.3001129627227783 and time: 7.149999229113261\n",
            "epoch 61 and loss is: 0.30076131224632263 and time: 7.264344799518585\n",
            "epoch 62 and loss is: 0.2995143532752991 and time: 7.381992673873901\n",
            "epoch 63 and loss is: 0.30063483119010925 and time: 7.499396761258443\n",
            "epoch 64 and loss is: 0.3000711500644684 and time: 7.615034977595012\n",
            "epoch 65 and loss is: 0.3011017441749573 and time: 7.728470782438914\n",
            "epoch 66 and loss is: 0.29974669218063354 and time: 7.84722497065862\n",
            "epoch 67 and loss is: 0.3002496063709259 and time: 7.963988788922628\n",
            "epoch 68 and loss is: 0.2989349663257599 and time: 8.082135967413585\n",
            "epoch 69 and loss is: 0.2992076873779297 and time: 8.199874778588613\n",
            "epoch 70 and loss is: 0.29906272888183594 and time: 8.319016428788503\n",
            "epoch 71 and loss is: 0.29745766520500183 and time: 8.436543269952137\n",
            "epoch 72 and loss is: 0.2971089482307434 and time: 8.553620970249176\n",
            "epoch 73 and loss is: 0.29709070920944214 and time: 8.67112443447113\n",
            "epoch 74 and loss is: 0.2978065311908722 and time: 8.785636428991953\n",
            "epoch 75 and loss is: 0.29789143800735474 and time: 8.904835760593414\n",
            "epoch 76 and loss is: 0.2965470552444458 and time: 9.022708443800608\n",
            "epoch 77 and loss is: 0.29745370149612427 and time: 9.14015398422877\n",
            "epoch 78 and loss is: 0.2962892949581146 and time: 9.255880264441172\n",
            "epoch 79 and loss is: 0.2972410023212433 and time: 9.374236746629078\n",
            "epoch 80 and loss is: 0.2966485321521759 and time: 9.490156094233194\n",
            "epoch 81 and loss is: 0.2956129312515259 and time: 9.606930812199911\n",
            "epoch 82 and loss is: 0.2964017689228058 and time: 9.724201202392578\n",
            "epoch 83 and loss is: 0.2974395453929901 and time: 9.842607549826305\n",
            "epoch 84 and loss is: 0.29497528076171875 and time: 9.961008763313293\n",
            "epoch 85 and loss is: 0.2950132489204407 and time: 10.081695512930553\n",
            "epoch 86 and loss is: 0.29554831981658936 and time: 10.199491584300995\n",
            "epoch 87 and loss is: 0.29693707823753357 and time: 10.31808834473292\n",
            "epoch 88 and loss is: 0.2958487570285797 and time: 10.4367662747701\n",
            "epoch 89 and loss is: 0.29553279280662537 and time: 10.553671459356943\n",
            "epoch 90 and loss is: 0.29449400305747986 and time: 10.672076106071472\n",
            "epoch 91 and loss is: 0.29608091711997986 and time: 10.79121530453364\n",
            "epoch 92 and loss is: 0.29509684443473816 and time: 10.911204516887665\n",
            "epoch 93 and loss is: 0.29340460896492004 and time: 11.031165484587351\n",
            "epoch 94 and loss is: 0.29510316252708435 and time: 11.15004817644755\n",
            "epoch 95 and loss is: 0.2951124906539917 and time: 11.26889895995458\n",
            "epoch 96 and loss is: 0.2939102053642273 and time: 11.385561279455821\n",
            "epoch 97 and loss is: 0.29309526085853577 and time: 11.502669401963551\n",
            "epoch 98 and loss is: 0.29287266731262207 and time: 11.621007732550304\n",
            "epoch 99 and loss is: 0.2925166189670563 and time: 11.73958462079366\n",
            "epoch 100 and loss is: 0.2941855788230896 and time: 11.855446728070577\n",
            "epoch 101 and loss is: 0.2936784625053406 and time: 11.97057117621104\n",
            "epoch 102 and loss is: 0.2928076982498169 and time: 12.090289648373922\n",
            "epoch 103 and loss is: 0.29270294308662415 and time: 12.209125912189483\n",
            "epoch 104 and loss is: 0.2929956018924713 and time: 12.327650984128317\n",
            "epoch 105 and loss is: 0.2917683720588684 and time: 12.443974661827088\n",
            "epoch 106 and loss is: 0.292410284280777 and time: 12.561052513122558\n",
            "epoch 107 and loss is: 0.2917690873146057 and time: 12.680475413799286\n",
            "epoch 108 and loss is: 0.2918797433376312 and time: 12.800436703364054\n",
            "epoch 109 and loss is: 0.2927446663379669 and time: 12.917844009399413\n",
            "epoch 110 and loss is: 0.29139283299446106 and time: 13.034199786186218\n",
            "epoch 111 and loss is: 0.2906704545021057 and time: 13.151487922668457\n",
            "epoch 112 and loss is: 0.2913099527359009 and time: 13.271485634644826\n",
            "epoch 113 and loss is: 0.2910253405570984 and time: 13.388882839679718\n",
            "epoch 114 and loss is: 0.29098764061927795 and time: 13.508683602015177\n",
            "epoch 115 and loss is: 0.29152703285217285 and time: 13.626704768339794\n",
            "epoch 116 and loss is: 0.289787620306015 and time: 13.746374547481537\n",
            "epoch 117 and loss is: 0.2915974259376526 and time: 13.864952997366588\n",
            "epoch 118 and loss is: 0.29130616784095764 and time: 13.984502470493316\n",
            "epoch 119 and loss is: 0.29070916771888733 and time: 14.102214233080547\n",
            "epoch 120 and loss is: 0.2893063426017761 and time: 14.222091484069825\n",
            "epoch 121 and loss is: 0.2900967299938202 and time: 14.34167519013087\n",
            "epoch 122 and loss is: 0.2905203402042389 and time: 14.460871386528016\n",
            "epoch 123 and loss is: 0.2890973389148712 and time: 14.57817629178365\n",
            "epoch 124 and loss is: 0.28844016790390015 and time: 14.693516794840495\n",
            "epoch 125 and loss is: 0.2900015711784363 and time: 14.81253079175949\n",
            "epoch 126 and loss is: 0.2895653247833252 and time: 14.932425133387248\n",
            "epoch 127 and loss is: 0.29003509879112244 and time: 15.050923625628153\n",
            "epoch 128 and loss is: 0.2893840968608856 and time: 15.170135764280955\n",
            "epoch 129 and loss is: 0.2892891764640808 and time: 15.28908150990804\n",
            "epoch 130 and loss is: 0.2892192006111145 and time: 15.40949722925822\n",
            "epoch 131 and loss is: 0.2887619435787201 and time: 15.529349680741628\n",
            "epoch 132 and loss is: 0.2882626950740814 and time: 15.648049227396648\n",
            "epoch 133 and loss is: 0.2875913381576538 and time: 15.768465121587118\n",
            "epoch 134 and loss is: 0.2884175479412079 and time: 15.88669771750768\n",
            "epoch 135 and loss is: 0.2888522148132324 and time: 16.004552646478018\n",
            "epoch 136 and loss is: 0.2871982753276825 and time: 16.123115825653077\n",
            "epoch 137 and loss is: 0.2878642678260803 and time: 16.243185659249622\n",
            "epoch 138 and loss is: 0.28729715943336487 and time: 16.3624742825826\n",
            "epoch 139 and loss is: 0.2883419692516327 and time: 16.478142090638478\n",
            "epoch 140 and loss is: 0.2885393798351288 and time: 16.595512620608012\n",
            "epoch 141 and loss is: 0.2888200879096985 and time: 16.714210903644563\n",
            "epoch 142 and loss is: 0.28644779324531555 and time: 16.832705028851827\n",
            "epoch 143 and loss is: 0.2865912914276123 and time: 16.949811812241872\n",
            "epoch 144 and loss is: 0.2865571677684784 and time: 17.070720636844634\n",
            "epoch 145 and loss is: 0.2883790135383606 and time: 17.189565896987915\n",
            "epoch 146 and loss is: 0.2861102819442749 and time: 17.310354435443877\n",
            "epoch 147 and loss is: 0.28715234994888306 and time: 17.42871419986089\n",
            "epoch 148 and loss is: 0.2866821885108948 and time: 17.546594536304475\n",
            "epoch 149 and loss is: 0.28676167130470276 and time: 17.666955522696178\n",
            "epoch 150 and loss is: 0.28659310936927795 and time: 17.784184980392457\n",
            "epoch 151 and loss is: 0.2868221700191498 and time: 17.903505249818167\n",
            "epoch 152 and loss is: 0.2865597903728485 and time: 18.024010852972665\n",
            "epoch 153 and loss is: 0.2859906554222107 and time: 18.142206358909608\n",
            "epoch 154 and loss is: 0.28524163365364075 and time: 18.261980211734773\n",
            "epoch 155 and loss is: 0.28578686714172363 and time: 18.379995612303414\n",
            "epoch 156 and loss is: 0.2868711054325104 and time: 18.497066756089527\n",
            "epoch 157 and loss is: 0.2861078679561615 and time: 18.614999651908875\n",
            "epoch 158 and loss is: 0.2869110107421875 and time: 18.734713582197823\n",
            "epoch 159 and loss is: 0.2851260304450989 and time: 18.852439125378925\n",
            "epoch 160 and loss is: 0.28554725646972656 and time: 18.96887807051341\n",
            "epoch 161 and loss is: 0.2858038544654846 and time: 19.086263958613078\n",
            "epoch 162 and loss is: 0.28494009375572205 and time: 19.205166522661845\n",
            "epoch 163 and loss is: 0.2855859398841858 and time: 19.32299597263336\n",
            "epoch 164 and loss is: 0.2852363586425781 and time: 19.439066747824352\n",
            "epoch 165 and loss is: 0.28469404578208923 and time: 19.55795790751775\n",
            "epoch 166 and loss is: 0.2860054671764374 and time: 19.67480708360672\n",
            "epoch 167 and loss is: 0.28697046637535095 and time: 19.793835008144377\n",
            "epoch 168 and loss is: 0.28542840480804443 and time: 19.911868127187095\n",
            "epoch 169 and loss is: 0.28544050455093384 and time: 20.031345391273497\n",
            "epoch 170 and loss is: 0.28457826375961304 and time: 20.151371737321217\n",
            "epoch 171 and loss is: 0.28440478444099426 and time: 20.268136584758757\n",
            "epoch 172 and loss is: 0.2852809727191925 and time: 20.386769966284433\n",
            "epoch 173 and loss is: 0.28494521975517273 and time: 20.50530001719793\n",
            "epoch 174 and loss is: 0.2840098738670349 and time: 20.621858417987823\n",
            "epoch 175 and loss is: 0.28403404355049133 and time: 20.740482087930044\n",
            "epoch 176 and loss is: 0.2829122245311737 and time: 20.859932247797648\n",
            "epoch 177 and loss is: 0.2837342321872711 and time: 20.975294637680054\n",
            "epoch 178 and loss is: 0.28365790843963623 and time: 21.09495598077774\n",
            "epoch 179 and loss is: 0.28400081396102905 and time: 21.212992640336356\n",
            "epoch 180 and loss is: 0.28308549523353577 and time: 21.33298402229945\n",
            "epoch 181 and loss is: 0.28324970602989197 and time: 21.453735550244648\n",
            "epoch 182 and loss is: 0.2826782166957855 and time: 21.570878843466442\n",
            "epoch 183 and loss is: 0.2831008732318878 and time: 21.689728995164234\n",
            "epoch 184 and loss is: 0.2826010286808014 and time: 21.80949242512385\n",
            "epoch 185 and loss is: 0.28362205624580383 and time: 21.928163206577302\n",
            "epoch 186 and loss is: 0.28312405943870544 and time: 22.04661387205124\n",
            "epoch 187 and loss is: 0.2834315896034241 and time: 22.165597867965698\n",
            "epoch 188 and loss is: 0.2840678095817566 and time: 22.28324259519577\n",
            "epoch 189 and loss is: 0.2826971411705017 and time: 22.401747481028238\n",
            "epoch 190 and loss is: 0.2828555405139923 and time: 22.522049860159555\n",
            "epoch 191 and loss is: 0.28280749917030334 and time: 22.636984392007193\n",
            "epoch 192 and loss is: 0.283021479845047 and time: 22.753790477911632\n",
            "epoch 193 and loss is: 0.2824498116970062 and time: 22.87142026424408\n",
            "epoch 194 and loss is: 0.2824923098087311 and time: 22.99228853782018\n",
            "epoch 195 and loss is: 0.28211864829063416 and time: 23.110495702425638\n",
            "epoch 196 and loss is: 0.2824491858482361 and time: 23.228792091210682\n",
            "epoch 197 and loss is: 0.2820867598056793 and time: 23.349121900399528\n",
            "epoch 198 and loss is: 0.28229618072509766 and time: 23.467789498964944\n",
            "epoch 199 and loss is: 0.28324705362319946 and time: 23.58686326344808\n",
            "epoch 200 and loss is: 0.28170067071914673 and time: 23.704241530100504\n",
            "epoch 201 and loss is: 0.28222814202308655 and time: 23.822831249237062\n",
            "epoch 202 and loss is: 0.2816244959831238 and time: 23.942279481887816\n",
            "epoch 203 and loss is: 0.2816663384437561 and time: 24.061164812246957\n",
            "epoch 204 and loss is: 0.2815346121788025 and time: 24.180054024855295\n",
            "epoch 205 and loss is: 0.2815333306789398 and time: 24.297525242964426\n",
            "epoch 206 and loss is: 0.281029611825943 and time: 24.41572228272756\n",
            "epoch 207 and loss is: 0.2821927070617676 and time: 24.534454600016275\n",
            "epoch 208 and loss is: 0.2820359766483307 and time: 24.65380573272705\n",
            "epoch 209 and loss is: 0.2805919945240021 and time: 24.77264606555303\n",
            "epoch 210 and loss is: 0.28113460540771484 and time: 24.89067485332489\n",
            "epoch 211 and loss is: 0.2808326184749603 and time: 25.009870251019795\n",
            "epoch 212 and loss is: 0.28198909759521484 and time: 25.1283095796903\n",
            "epoch 213 and loss is: 0.2814505994319916 and time: 25.24780354499817\n",
            "epoch 214 and loss is: 0.27976956963539124 and time: 25.367137595017752\n",
            "epoch 215 and loss is: 0.280912846326828 and time: 25.48529270887375\n",
            "epoch 216 and loss is: 0.2799832820892334 and time: 25.604538349310555\n",
            "epoch 217 and loss is: 0.28149986267089844 and time: 25.723630328973133\n",
            "epoch 218 and loss is: 0.2803959548473358 and time: 25.84325708548228\n",
            "epoch 219 and loss is: 0.2818446755409241 and time: 25.96231827735901\n",
            "epoch 220 and loss is: 0.28098148107528687 and time: 26.08088215589523\n",
            "epoch 221 and loss is: 0.28058671951293945 and time: 26.198805622259776\n",
            "epoch 222 and loss is: 0.28051939606666565 and time: 26.318715647856394\n",
            "epoch 223 and loss is: 0.2800876200199127 and time: 26.43748871088028\n",
            "epoch 224 and loss is: 0.2797718942165375 and time: 26.555325667063396\n",
            "epoch 225 and loss is: 0.2804923951625824 and time: 26.674763373533885\n",
            "epoch 226 and loss is: 0.28051266074180603 and time: 26.793714018662772\n",
            "epoch 227 and loss is: 0.2795293629169464 and time: 26.914521940549214\n",
            "epoch 228 and loss is: 0.28061729669570923 and time: 27.0340158144633\n",
            "epoch 229 and loss is: 0.279388427734375 and time: 27.151815096537273\n",
            "epoch 230 and loss is: 0.2795768082141876 and time: 27.26992017030716\n",
            "epoch 231 and loss is: 0.2791443467140198 and time: 27.388592799504597\n",
            "epoch 232 and loss is: 0.2790641784667969 and time: 27.50750062863032\n",
            "epoch 233 and loss is: 0.2798774242401123 and time: 27.62818195025126\n",
            "epoch 234 and loss is: 0.2800906300544739 and time: 27.74681271711985\n",
            "epoch 235 and loss is: 0.2795909643173218 and time: 27.865697558720907\n",
            "epoch 236 and loss is: 0.2797392010688782 and time: 27.980209441979728\n",
            "epoch 237 and loss is: 0.2793368697166443 and time: 28.100433540344238\n",
            "epoch 238 and loss is: 0.279958039522171 and time: 28.218358314037324\n",
            "epoch 239 and loss is: 0.2796066701412201 and time: 28.33782615661621\n",
            "epoch 240 and loss is: 0.27998292446136475 and time: 28.457596079508463\n",
            "epoch 241 and loss is: 0.2790907025337219 and time: 28.577490242322288\n",
            "epoch 242 and loss is: 0.2791286110877991 and time: 28.69604718287786\n",
            "epoch 243 and loss is: 0.27882394194602966 and time: 28.814914286136627\n",
            "epoch 244 and loss is: 0.27888911962509155 and time: 28.935393981138866\n",
            "epoch 245 and loss is: 0.27884072065353394 and time: 29.052665905157724\n",
            "epoch 246 and loss is: 0.27965909242630005 and time: 29.171637066205342\n",
            "epoch 247 and loss is: 0.2793167233467102 and time: 29.2910325050354\n",
            "epoch 248 and loss is: 0.27875733375549316 and time: 29.41142498254776\n",
            "epoch 249 and loss is: 0.2793309986591339 and time: 29.529552018642427\n",
            "epoch 250 and loss is: 0.27835813164711 and time: 29.6489714940389\n",
            "epoch 251 and loss is: 0.27902284264564514 and time: 29.767674493789674\n",
            "epoch 252 and loss is: 0.27843135595321655 and time: 29.887267867724102\n",
            "epoch 253 and loss is: 0.27896636724472046 and time: 30.006867039203645\n",
            "epoch 254 and loss is: 0.2776569128036499 and time: 30.12539500395457\n",
            "epoch 255 and loss is: 0.27874910831451416 and time: 30.24438817501068\n",
            "epoch 256 and loss is: 0.2785671055316925 and time: 30.361777114868165\n",
            "epoch 257 and loss is: 0.2771514654159546 and time: 30.4793638308843\n",
            "epoch 258 and loss is: 0.2782272696495056 and time: 30.596937199433643\n",
            "epoch 259 and loss is: 0.2775733172893524 and time: 30.713195459047952\n",
            "epoch 260 and loss is: 0.27809736132621765 and time: 30.831746705373128\n",
            "epoch 261 and loss is: 0.2783999741077423 and time: 30.951026984055837\n",
            "epoch 262 and loss is: 0.27831581234931946 and time: 31.067689283688864\n",
            "epoch 263 and loss is: 0.2777308225631714 and time: 31.186126045385997\n",
            "epoch 264 and loss is: 0.27827882766723633 and time: 31.303544501463573\n",
            "epoch 265 and loss is: 0.2773471176624298 and time: 31.421906451384228\n",
            "epoch 266 and loss is: 0.27760109305381775 and time: 31.540129923820494\n",
            "epoch 267 and loss is: 0.27668577432632446 and time: 31.657924195130665\n",
            "epoch 268 and loss is: 0.2775532305240631 and time: 31.778981924057007\n",
            "epoch 269 and loss is: 0.27687162160873413 and time: 31.89717390934626\n",
            "epoch 270 and loss is: 0.2765953540802002 and time: 32.014519516627\n",
            "epoch 271 and loss is: 0.27742141485214233 and time: 32.131400895118716\n",
            "epoch 272 and loss is: 0.2769237160682678 and time: 32.252426846822104\n",
            "epoch 273 and loss is: 0.27784305810928345 and time: 32.37074363231659\n",
            "epoch 274 and loss is: 0.27733445167541504 and time: 32.489514720439914\n",
            "epoch 275 and loss is: 0.27799805998802185 and time: 32.60826981862386\n",
            "epoch 276 and loss is: 0.27680322527885437 and time: 32.72802499135335\n",
            "epoch 277 and loss is: 0.277316689491272 and time: 32.84879581133524\n",
            "epoch 278 and loss is: 0.27681732177734375 and time: 32.9688612262408\n",
            "epoch 279 and loss is: 0.27775147557258606 and time: 33.08537102937699\n",
            "epoch 280 and loss is: 0.2772781550884247 and time: 33.2035368680954\n",
            "epoch 281 and loss is: 0.27625608444213867 and time: 33.32108452320099\n",
            "epoch 282 and loss is: 0.2764168679714203 and time: 33.44147025346756\n",
            "epoch 283 and loss is: 0.27735766768455505 and time: 33.560441565513614\n",
            "epoch 284 and loss is: 0.27565091848373413 and time: 33.67867314815521\n",
            "epoch 285 and loss is: 0.2768324017524719 and time: 33.79630858898163\n",
            "epoch 286 and loss is: 0.27660202980041504 and time: 33.91585157314936\n",
            "epoch 287 and loss is: 0.2759692668914795 and time: 34.03108047644297\n",
            "epoch 288 and loss is: 0.2758631110191345 and time: 34.15108189582825\n",
            "epoch 289 and loss is: 0.27619391679763794 and time: 34.270182037353514\n",
            "epoch 290 and loss is: 0.27612459659576416 and time: 34.38818452358246\n",
            "epoch 291 and loss is: 0.27698177099227905 and time: 34.50798457066218\n",
            "epoch 292 and loss is: 0.27611252665519714 and time: 34.62756818135579\n",
            "epoch 293 and loss is: 0.27575716376304626 and time: 34.74476913611094\n",
            "epoch 294 and loss is: 0.27578145265579224 and time: 34.862419323126474\n",
            "epoch 295 and loss is: 0.27473920583724976 and time: 34.98245979547501\n",
            "epoch 296 and loss is: 0.27586424350738525 and time: 35.10344775517782\n",
            "epoch 297 and loss is: 0.2748474180698395 and time: 35.22293516397476\n",
            "epoch 298 and loss is: 0.2759005129337311 and time: 35.34213738441467\n",
            "epoch 299 and loss is: 0.2764357626438141 and time: 35.45828179915746\n",
            "epoch 300 and loss is: 0.2756185531616211 and time: 35.57711912790934\n",
            "epoch 301 and loss is: 0.27562379837036133 and time: 35.69505483706792\n",
            "epoch 302 and loss is: 0.2760109603404999 and time: 35.813799460728966\n",
            "epoch 303 and loss is: 0.27576717734336853 and time: 35.93249899148941\n",
            "epoch 304 and loss is: 0.2747599482536316 and time: 36.05036855936051\n",
            "epoch 305 and loss is: 0.27501967549324036 and time: 36.16993856827418\n",
            "epoch 306 and loss is: 0.2760053277015686 and time: 36.28720450798671\n",
            "epoch 307 and loss is: 0.2755856513977051 and time: 36.40573478539785\n",
            "epoch 308 and loss is: 0.27507948875427246 and time: 36.52495938539505\n",
            "epoch 309 and loss is: 0.2757505178451538 and time: 36.64373540878296\n",
            "epoch 310 and loss is: 0.2740291655063629 and time: 36.762247208754225\n",
            "epoch 311 and loss is: 0.27495962381362915 and time: 36.87710380554199\n",
            "epoch 312 and loss is: 0.2756099998950958 and time: 36.99796335697174\n",
            "epoch 313 and loss is: 0.2759271264076233 and time: 37.11566951274872\n",
            "epoch 314 and loss is: 0.2757773995399475 and time: 37.23622907002767\n",
            "epoch 315 and loss is: 0.2753712832927704 and time: 37.35506317615509\n",
            "epoch 316 and loss is: 0.2754591703414917 and time: 37.47534504334132\n",
            "epoch 317 and loss is: 0.27430057525634766 and time: 37.59504857858022\n",
            "epoch 318 and loss is: 0.2744024097919464 and time: 37.71410337686539\n",
            "epoch 319 and loss is: 0.27505362033843994 and time: 37.834545274575554\n",
            "epoch 320 and loss is: 0.2750628590583801 and time: 37.95779905319214\n",
            "epoch 321 and loss is: 0.2748914957046509 and time: 38.079088759422305\n",
            "epoch 322 and loss is: 0.2748396694660187 and time: 38.19744720856349\n",
            "epoch 323 and loss is: 0.274876207113266 and time: 38.318262084325156\n",
            "epoch 324 and loss is: 0.2738337516784668 and time: 38.43707062005997\n",
            "epoch 325 and loss is: 0.2743902802467346 and time: 38.557694594065346\n",
            "epoch 326 and loss is: 0.27456140518188477 and time: 38.67783420483271\n",
            "epoch 327 and loss is: 0.27473679184913635 and time: 38.79843691984812\n",
            "epoch 328 and loss is: 0.27450400590896606 and time: 38.91956386963526\n",
            "epoch 329 and loss is: 0.27425506711006165 and time: 39.03966217835744\n",
            "epoch 330 and loss is: 0.27388954162597656 and time: 39.158077561855315\n",
            "epoch 331 and loss is: 0.2738945782184601 and time: 39.27896913290024\n",
            "epoch 332 and loss is: 0.2742290198802948 and time: 39.39897057612737\n",
            "epoch 333 and loss is: 0.2740538716316223 and time: 39.51736402511597\n",
            "epoch 334 and loss is: 0.27440157532691956 and time: 39.63561517397563\n",
            "epoch 335 and loss is: 0.27324995398521423 and time: 39.75203728675842\n",
            "epoch 336 and loss is: 0.27480757236480713 and time: 39.8705291668574\n",
            "epoch 337 and loss is: 0.27388960123062134 and time: 39.99051754077276\n",
            "epoch 338 and loss is: 0.2744685113430023 and time: 40.1108966310819\n",
            "epoch 339 and loss is: 0.27335575222969055 and time: 40.22935346762339\n",
            "epoch 340 and loss is: 0.2734699547290802 and time: 40.34970489740372\n",
            "epoch 341 and loss is: 0.2733549475669861 and time: 40.46777993837993\n",
            "epoch 342 and loss is: 0.27413442730903625 and time: 40.58777792453766\n",
            "epoch 343 and loss is: 0.2739941477775574 and time: 40.706573669115706\n",
            "epoch 344 and loss is: 0.2729162275791168 and time: 40.824970352649686\n",
            "epoch 345 and loss is: 0.2736627161502838 and time: 40.94749942223231\n",
            "epoch 346 and loss is: 0.27336376905441284 and time: 41.0663929661115\n",
            "epoch 347 and loss is: 0.2737382650375366 and time: 41.18318906625112\n",
            "epoch 348 and loss is: 0.27414610981941223 and time: 41.30358777046204\n",
            "epoch 349 and loss is: 0.2734018564224243 and time: 41.42554775079091\n",
            "epoch 350 and loss is: 0.27399784326553345 and time: 41.54600702524185\n",
            "epoch 351 and loss is: 0.27299588918685913 and time: 41.66323830286662\n",
            "epoch 352 and loss is: 0.2729845941066742 and time: 41.78280982176463\n",
            "epoch 353 and loss is: 0.27332696318626404 and time: 41.9019230167071\n",
            "epoch 354 and loss is: 0.273322731256485 and time: 42.020753689606984\n",
            "epoch 355 and loss is: 0.272209107875824 and time: 42.14100228150686\n",
            "epoch 356 and loss is: 0.27225396037101746 and time: 42.26020362774531\n",
            "epoch 357 and loss is: 0.273161381483078 and time: 42.37923875252406\n",
            "epoch 358 and loss is: 0.272635817527771 and time: 42.49849511782328\n",
            "epoch 359 and loss is: 0.27320536971092224 and time: 42.61873580217362\n",
            "epoch 360 and loss is: 0.2735331356525421 and time: 42.73871759573618\n",
            "epoch 361 and loss is: 0.2740027904510498 and time: 42.85786688327789\n",
            "epoch 362 and loss is: 0.27332940697669983 and time: 42.974730690320335\n",
            "epoch 363 and loss is: 0.2731647193431854 and time: 43.09776775042216\n",
            "epoch 364 and loss is: 0.27241751551628113 and time: 43.22062322298686\n",
            "epoch 365 and loss is: 0.2724686563014984 and time: 43.34302384853363\n",
            "epoch 366 and loss is: 0.27295389771461487 and time: 43.46493793328603\n",
            "epoch 367 and loss is: 0.2726399600505829 and time: 43.58218857049942\n",
            "epoch 368 and loss is: 0.27260664105415344 and time: 43.701778630415596\n",
            "epoch 369 and loss is: 0.27179449796676636 and time: 43.82145485877991\n",
            "epoch 370 and loss is: 0.2729581594467163 and time: 43.940902372201286\n",
            "epoch 371 and loss is: 0.27228087186813354 and time: 44.06144050359726\n",
            "epoch 372 and loss is: 0.2728188931941986 and time: 44.18162535826365\n",
            "epoch 373 and loss is: 0.272162526845932 and time: 44.302280096213025\n",
            "epoch 374 and loss is: 0.27284786105155945 and time: 44.42141785224279\n",
            "epoch 375 and loss is: 0.272995263338089 and time: 44.54293561776479\n",
            "epoch 376 and loss is: 0.2727643847465515 and time: 44.66315480470657\n",
            "epoch 377 and loss is: 0.2722986042499542 and time: 44.78430354595184\n",
            "epoch 378 and loss is: 0.272555410861969 and time: 44.90518862406413\n",
            "epoch 379 and loss is: 0.272693008184433 and time: 45.02472699085872\n",
            "epoch 380 and loss is: 0.27273809909820557 and time: 45.14656769434611\n",
            "epoch 381 and loss is: 0.27271127700805664 and time: 45.267963886260986\n",
            "epoch 382 and loss is: 0.27318254113197327 and time: 45.38930782477061\n",
            "epoch 383 and loss is: 0.2726278603076935 and time: 45.50967110792796\n",
            "epoch 384 and loss is: 0.2723196744918823 and time: 45.62965366442998\n",
            "epoch 385 and loss is: 0.2720799148082733 and time: 45.74887378613154\n",
            "epoch 386 and loss is: 0.2717685401439667 and time: 45.86851666768392\n",
            "epoch 387 and loss is: 0.27234506607055664 and time: 45.98697426319122\n",
            "epoch 388 and loss is: 0.2727597653865814 and time: 46.105804816881815\n",
            "epoch 389 and loss is: 0.2717316150665283 and time: 46.226129492123924\n",
            "epoch 390 and loss is: 0.27249568700790405 and time: 46.34713628689448\n",
            "epoch 391 and loss is: 0.2721240818500519 and time: 46.46820457776388\n",
            "epoch 392 and loss is: 0.2721208333969116 and time: 46.58978348970413\n",
            "epoch 393 and loss is: 0.27163854241371155 and time: 46.71139175494512\n",
            "epoch 394 and loss is: 0.271605521440506 and time: 46.83192329009374\n",
            "epoch 395 and loss is: 0.27257364988327026 and time: 46.95107777118683\n",
            "epoch 396 and loss is: 0.2719573676586151 and time: 47.07271267970403\n",
            "epoch 397 and loss is: 0.2724754810333252 and time: 47.194773141543074\n",
            "epoch 398 and loss is: 0.2716091275215149 and time: 47.316173438231154\n",
            "epoch 399 and loss is: 0.27166748046875 and time: 47.43799039522807\n",
            "epoch 400 and loss is: 0.2717990577220917 and time: 47.55923699935277\n",
            "epoch 401 and loss is: 0.2722453474998474 and time: 47.679893386363986\n",
            "epoch 402 and loss is: 0.2715751528739929 and time: 47.80172652403514\n",
            "epoch 403 and loss is: 0.27220818400382996 and time: 47.92653679052989\n",
            "epoch 404 and loss is: 0.27192422747612 and time: 48.048608613014224\n",
            "epoch 405 and loss is: 0.27104079723358154 and time: 48.17154452800751\n",
            "epoch 406 and loss is: 0.2712481617927551 and time: 48.29623538653056\n",
            "epoch 407 and loss is: 0.2718258202075958 and time: 48.417656473318736\n",
            "epoch 408 and loss is: 0.2711864113807678 and time: 48.5376686056455\n",
            "epoch 409 and loss is: 0.2716384530067444 and time: 48.65801486174266\n",
            "epoch 410 and loss is: 0.2723482549190521 and time: 48.777186187108356\n",
            "epoch 411 and loss is: 0.2719140946865082 and time: 48.895158632596335\n",
            "epoch 412 and loss is: 0.27040645480155945 and time: 49.01594314177831\n",
            "epoch 413 and loss is: 0.2713460624217987 and time: 49.13548410733541\n",
            "epoch 414 and loss is: 0.271474152803421 and time: 49.254246870676674\n",
            "epoch 415 and loss is: 0.27124831080436707 and time: 49.37318486769994\n",
            "epoch 416 and loss is: 0.2715291678905487 and time: 49.49229329029719\n",
            "epoch 417 and loss is: 0.2717990279197693 and time: 49.61043882369995\n",
            "epoch 418 and loss is: 0.2715964913368225 and time: 49.729765558242796\n",
            "epoch 419 and loss is: 0.2707720398902893 and time: 49.848275593916576\n",
            "epoch 420 and loss is: 0.27043893933296204 and time: 49.96794947385788\n",
            "epoch 421 and loss is: 0.2713783383369446 and time: 50.08809885581334\n",
            "epoch 422 and loss is: 0.27106189727783203 and time: 50.207319156328836\n",
            "epoch 423 and loss is: 0.2717381715774536 and time: 50.32914195458094\n",
            "epoch 424 and loss is: 0.27043625712394714 and time: 50.44832552671433\n",
            "epoch 425 and loss is: 0.2718171775341034 and time: 50.56677287419637\n",
            "epoch 426 and loss is: 0.27043941617012024 and time: 50.685692846775055\n",
            "epoch 427 and loss is: 0.2718620300292969 and time: 50.80714290539424\n",
            "epoch 428 and loss is: 0.27107733488082886 and time: 50.926684753100076\n",
            "epoch 429 and loss is: 0.2706485092639923 and time: 51.0456764737765\n",
            "epoch 430 and loss is: 0.2697310447692871 and time: 51.165655024846394\n",
            "epoch 431 and loss is: 0.2711604833602905 and time: 51.28488935232163\n",
            "epoch 432 and loss is: 0.2710222005844116 and time: 51.40545573234558\n",
            "epoch 433 and loss is: 0.271502286195755 and time: 51.52355029582977\n",
            "epoch 434 and loss is: 0.2714080214500427 and time: 51.63833525975545\n",
            "epoch 435 and loss is: 0.2709731161594391 and time: 51.75690448681514\n",
            "epoch 436 and loss is: 0.2714710831642151 and time: 51.8756121635437\n",
            "epoch 437 and loss is: 0.2701614499092102 and time: 51.99465750455856\n",
            "epoch 438 and loss is: 0.27143821120262146 and time: 52.111458039283754\n",
            "epoch 439 and loss is: 0.272182434797287 and time: 52.22949508031209\n",
            "epoch 440 and loss is: 0.27054062485694885 and time: 52.34970477024714\n",
            "epoch 441 and loss is: 0.27148139476776123 and time: 52.46897033055623\n",
            "epoch 442 and loss is: 0.271427184343338 and time: 52.58724136352539\n",
            "epoch 443 and loss is: 0.27167683839797974 and time: 52.70553036133448\n",
            "epoch 444 and loss is: 0.27124857902526855 and time: 52.823588828245796\n",
            "epoch 445 and loss is: 0.2712515890598297 and time: 52.93940165440242\n",
            "epoch 446 and loss is: 0.27176806330680847 and time: 53.056403398513794\n",
            "epoch 447 and loss is: 0.2707279920578003 and time: 53.174346721172334\n",
            "epoch 448 and loss is: 0.2707087993621826 and time: 53.29216983318329\n",
            "epoch 449 and loss is: 0.2697666883468628 and time: 53.41380274295807\n",
            "Training took 53.484009273846944 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooqFtx7tJnTB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "5536eb71-b216-41fa-c26f-e415b6da296b"
      },
      "source": [
        "#Plotting our error\n",
        "plt.plot(range(epochs),losses)\n",
        "plt.ylabel('CrossEntropy Loss')\n",
        "plt.xlabel('Epoch') "
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Epoch')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mk1YLKANy1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c4fa13d-8c41-457f-aea4-dee9456429da"
      },
      "source": [
        "#This just turns off the backpropagation, so we can use the model for evaluation rather than training. This helps reduce memory usage and computation speed\n",
        "with torch.no_grad():\n",
        "  y_val = model.forward(cat_test, cont_test)\n",
        "  loss = criterion(y_val, y_test) #comparing the actual results with the evaluation results\n",
        "\n",
        "print('Cross Entropy loss =',loss) #average cross entropy loss"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross Entropy loss = tensor(0.2720)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7ZnyCFpOTjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab39ccf3-fb44-4f1f-ab85-b92fbfe8f717"
      },
      "source": [
        "rows = 50\n",
        "correct = 0\n",
        "print(f'{\"MODEL OUTPUT\":26} ARGMAX  Y_TEST')\n",
        "for i in range(rows):\n",
        "    print(f'{str(y_val[i]):26} {y_val[i].argmax():^7}{y_test[i]:^7}')\n",
        "    if y_val[i].argmax().item() == y_test[i]:\n",
        "        correct += 1\n",
        "print(f'\\n{correct} out of {rows} = {100*correct/rows:.2f}% correct')"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MODEL OUTPUT               ARGMAX  Y_TEST\n",
            "tensor([ 0.5733, -1.0053])    0      0   \n",
            "tensor([-1.5124,  1.4113])    1      1   \n",
            "tensor([-0.2379,  0.1584])    1      1   \n",
            "tensor([ 0.9395, -2.1842])    0      0   \n",
            "tensor([ 0.1371, -1.1264])    0      0   \n",
            "tensor([ 1.7301, -0.1443])    0      1   \n",
            "tensor([ 1.6748, -1.3487])    0      0   \n",
            "tensor([ 2.4151, -1.9125])    0      0   \n",
            "tensor([-1.4529,  1.8120])    1      1   \n",
            "tensor([ 0.7032, -0.8852])    0      0   \n",
            "tensor([ 3.1385, -1.5614])    0      0   \n",
            "tensor([ 2.0131, -0.9436])    0      0   \n",
            "tensor([-0.3525, -0.2681])    1      0   \n",
            "tensor([-0.8683,  1.9228])    1      1   \n",
            "tensor([ 1.8537, -1.1957])    0      0   \n",
            "tensor([-0.1383,  2.1959])    1      1   \n",
            "tensor([ 1.8637, -1.5928])    0      1   \n",
            "tensor([ 2.1457, -1.0428])    0      1   \n",
            "tensor([-2.4175,  3.8597])    1      1   \n",
            "tensor([ 0.5407, -1.1942])    0      0   \n",
            "tensor([ 0.6759, -1.5295])    0      1   \n",
            "tensor([-1.2102,  1.1387])    1      1   \n",
            "tensor([ 2.3851, -1.9046])    0      0   \n",
            "tensor([-1.0291,  1.0045])    1      1   \n",
            "tensor([ 1.8126, -1.4829])    0      0   \n",
            "tensor([ 2.1509, -1.3798])    0      0   \n",
            "tensor([ 1.6812, -1.7918])    0      0   \n",
            "tensor([-1.8686,  1.5983])    1      1   \n",
            "tensor([ 1.9992, -3.6356])    0      0   \n",
            "tensor([-1.9017,  0.8782])    1      1   \n",
            "tensor([-1.2309,  1.0907])    1      0   \n",
            "tensor([ 1.5103, -1.3736])    0      0   \n",
            "tensor([ 1.4125, -2.1064])    0      0   \n",
            "tensor([ 1.5995, -1.4385])    0      0   \n",
            "tensor([-1.1398,  1.0628])    1      1   \n",
            "tensor([-1.0691,  3.0585])    1      1   \n",
            "tensor([ 0.9023, -0.4979])    0      0   \n",
            "tensor([-1.8882,  1.0307])    1      1   \n",
            "tensor([ 2.1625, -0.5888])    0      0   \n",
            "tensor([ 2.0362, -1.1145])    0      0   \n",
            "tensor([ 0.7337, -1.8467])    0      0   \n",
            "tensor([ 1.4759, -1.8915])    0      0   \n",
            "tensor([-0.4324, -0.4548])    0      1   \n",
            "tensor([ 1.8486, -1.7765])    0      0   \n",
            "tensor([ 0.1771, -1.0422])    0      0   \n",
            "tensor([-0.0981, -1.2713])    0      0   \n",
            "tensor([-1.3606,  0.0316])    1      1   \n",
            "tensor([ 0.3604, -1.6245])    0      0   \n",
            "tensor([ 1.9978, -1.5519])    0      0   \n",
            "tensor([-0.3752, -0.7629])    0      0   \n",
            "\n",
            "43 out of 50 = 86.00% correct\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNpeeH0HPU0M"
      },
      "source": [
        "torch.save(model.state_dict(), 'taxiFareClass_model.pt') #saving the model"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIrFRnXVkPG-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}