{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Github_Full ANN Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ8ElxAElgyx"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml8dgZJlCFAk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "a527c30b-867b-4760-adcf-fe0f4b634adc"
      },
      "source": [
        "df = pd.read_csv('NYCTaxiFares.csv')\n",
        "df #here we're try to predict the fare_amount column "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>fare_class</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010-04-19 08:17:56 UTC</td>\n",
              "      <td>6.5</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.992365</td>\n",
              "      <td>40.730521</td>\n",
              "      <td>-73.975499</td>\n",
              "      <td>40.744746</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-04-17 15:43:53 UTC</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.990078</td>\n",
              "      <td>40.740558</td>\n",
              "      <td>-73.974232</td>\n",
              "      <td>40.744114</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2010-04-17 11:23:26 UTC</td>\n",
              "      <td>10.1</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.994149</td>\n",
              "      <td>40.751118</td>\n",
              "      <td>-73.960064</td>\n",
              "      <td>40.766235</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2010-04-11 21:25:03 UTC</td>\n",
              "      <td>8.9</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.990485</td>\n",
              "      <td>40.756422</td>\n",
              "      <td>-73.971205</td>\n",
              "      <td>40.748192</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-04-17 02:19:01 UTC</td>\n",
              "      <td>19.7</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.990976</td>\n",
              "      <td>40.734202</td>\n",
              "      <td>-73.905956</td>\n",
              "      <td>40.743115</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119995</th>\n",
              "      <td>2010-04-18 14:33:03 UTC</td>\n",
              "      <td>15.3</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.955857</td>\n",
              "      <td>40.784590</td>\n",
              "      <td>-73.981941</td>\n",
              "      <td>40.736789</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119996</th>\n",
              "      <td>2010-04-23 10:27:48 UTC</td>\n",
              "      <td>15.3</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.996329</td>\n",
              "      <td>40.772727</td>\n",
              "      <td>-74.049890</td>\n",
              "      <td>40.740413</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119997</th>\n",
              "      <td>2010-04-18 18:50:40 UTC</td>\n",
              "      <td>12.5</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.988574</td>\n",
              "      <td>40.749772</td>\n",
              "      <td>-74.011541</td>\n",
              "      <td>40.707799</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119998</th>\n",
              "      <td>2010-04-13 08:14:44 UTC</td>\n",
              "      <td>4.9</td>\n",
              "      <td>0</td>\n",
              "      <td>-74.004449</td>\n",
              "      <td>40.724529</td>\n",
              "      <td>-73.992697</td>\n",
              "      <td>40.730765</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119999</th>\n",
              "      <td>2010-04-17 16:00:14 UTC</td>\n",
              "      <td>5.3</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.955415</td>\n",
              "      <td>40.771920</td>\n",
              "      <td>-73.967623</td>\n",
              "      <td>40.763015</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120000 rows Ã— 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                pickup_datetime  fare_amount  ...  dropoff_latitude  passenger_count\n",
              "0       2010-04-19 08:17:56 UTC          6.5  ...         40.744746                1\n",
              "1       2010-04-17 15:43:53 UTC          6.9  ...         40.744114                1\n",
              "2       2010-04-17 11:23:26 UTC         10.1  ...         40.766235                2\n",
              "3       2010-04-11 21:25:03 UTC          8.9  ...         40.748192                1\n",
              "4       2010-04-17 02:19:01 UTC         19.7  ...         40.743115                1\n",
              "...                         ...          ...  ...               ...              ...\n",
              "119995  2010-04-18 14:33:03 UTC         15.3  ...         40.736789                1\n",
              "119996  2010-04-23 10:27:48 UTC         15.3  ...         40.740413                1\n",
              "119997  2010-04-18 18:50:40 UTC         12.5  ...         40.707799                3\n",
              "119998  2010-04-13 08:14:44 UTC          4.9  ...         40.730765                1\n",
              "119999  2010-04-17 16:00:14 UTC          5.3  ...         40.763015                3\n",
              "\n",
              "[120000 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ2snS_cCpI9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420a1c38-d87d-4cee-8904-e41d27b3088b"
      },
      "source": [
        "df['fare_amount'].describe() #looks like the largest value was $49.90 and the average price is $10.04"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    120000.000000\n",
              "mean         10.040326\n",
              "std           7.500134\n",
              "min           2.500000\n",
              "25%           5.700000\n",
              "50%           7.700000\n",
              "75%          11.300000\n",
              "max          49.900000\n",
              "Name: fare_amount, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEIuCChUGPH7"
      },
      "source": [
        "# **Part 1**\n",
        "Feature Engineering 1: Distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIBLtDGWCUVS"
      },
      "source": [
        "#Below is the Haversine distance formula to calculate the distance travelled on a sphere. So in our case, we'd be using it to calculate distance between the pick up/drop off longitudes/latitudes on Earth\r\n",
        "def Haversine(dataframe, pickup_lat, pickup_long, dropoff_lat, dropoff_long):\r\n",
        "    r = 6371  #average radius in km of Earth\r\n",
        "       \r\n",
        "    phi1 = np.radians(df[pickup_lat]) #converting into radians\r\n",
        "    phi2 = np.radians(df[dropoff_lat]) #converting into radians\r\n",
        "    \r\n",
        "    delta_phi = np.radians(df[dropoff_lat]-df[pickup_lat]) #converting into radians\r\n",
        "    delta_lambda = np.radians(df[dropoff_long]-df[pickup_long]) #converting into radians\r\n",
        "     \r\n",
        "    a = np.sin(delta_phi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda/2)**2 #Haversine distance formula\r\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\r\n",
        "    d = (r * c) # in kilometers\r\n",
        "\r\n",
        "    return d"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRz-9k0zEfh-"
      },
      "source": [
        "df['dist_km'] = Haversine(df, 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude') #adding a new column to contain the distance calculated by the haversine function"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5opgPT3Evd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "0fd014f0-e732-47b5-9c41-e5a3d40444c2"
      },
      "source": [
        "#We just performed Feature Engineering\n",
        "#Feature Engineering is taking the features we are already given and creating a new one that will be more useful for our model. In this case, we feature engineered the dist_km column using our latitude and longitude data\n",
        "#The reason we created this dist_km column is because if we tried creating our model that took in just the longitude/latitude, that's a lot more data to process and the extra variables may result in poor results. In addition, the longitude/latitude data points are not too big of a difference from each other. As such, creating a new, distinct single feature (dist_km) could result in faster computation and better results.\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>fare_class</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>dist_km</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010-04-19 08:17:56 UTC</td>\n",
              "      <td>6.5</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.992365</td>\n",
              "      <td>40.730521</td>\n",
              "      <td>-73.975499</td>\n",
              "      <td>40.744746</td>\n",
              "      <td>1</td>\n",
              "      <td>2.126312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-04-17 15:43:53 UTC</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.990078</td>\n",
              "      <td>40.740558</td>\n",
              "      <td>-73.974232</td>\n",
              "      <td>40.744114</td>\n",
              "      <td>1</td>\n",
              "      <td>1.392307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2010-04-17 11:23:26 UTC</td>\n",
              "      <td>10.1</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.994149</td>\n",
              "      <td>40.751118</td>\n",
              "      <td>-73.960064</td>\n",
              "      <td>40.766235</td>\n",
              "      <td>2</td>\n",
              "      <td>3.326763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2010-04-11 21:25:03 UTC</td>\n",
              "      <td>8.9</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.990485</td>\n",
              "      <td>40.756422</td>\n",
              "      <td>-73.971205</td>\n",
              "      <td>40.748192</td>\n",
              "      <td>1</td>\n",
              "      <td>1.864129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-04-17 02:19:01 UTC</td>\n",
              "      <td>19.7</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.990976</td>\n",
              "      <td>40.734202</td>\n",
              "      <td>-73.905956</td>\n",
              "      <td>40.743115</td>\n",
              "      <td>1</td>\n",
              "      <td>7.231321</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           pickup_datetime  fare_amount  ...  passenger_count   dist_km\n",
              "0  2010-04-19 08:17:56 UTC          6.5  ...                1  2.126312\n",
              "1  2010-04-17 15:43:53 UTC          6.9  ...                1  1.392307\n",
              "2  2010-04-17 11:23:26 UTC         10.1  ...                2  3.326763\n",
              "3  2010-04-11 21:25:03 UTC          8.9  ...                1  1.864129\n",
              "4  2010-04-17 02:19:01 UTC         19.7  ...                1  7.231321\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjbfz-dIGSRz"
      },
      "source": [
        "Feature Engineering 2: Date-Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O5gQLy3Ev_J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d04e91a-8679-437f-e214-856cd001285f"
      },
      "source": [
        "df.info() #notice how pickup_datetime has Dtype = object, meaning it's treated like a string right now"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 120000 entries, 0 to 119999\n",
            "Data columns (total 9 columns):\n",
            " #   Column             Non-Null Count   Dtype  \n",
            "---  ------             --------------   -----  \n",
            " 0   pickup_datetime    120000 non-null  object \n",
            " 1   fare_amount        120000 non-null  float64\n",
            " 2   fare_class         120000 non-null  int64  \n",
            " 3   pickup_longitude   120000 non-null  float64\n",
            " 4   pickup_latitude    120000 non-null  float64\n",
            " 5   dropoff_longitude  120000 non-null  float64\n",
            " 6   dropoff_latitude   120000 non-null  float64\n",
            " 7   passenger_count    120000 non-null  int64  \n",
            " 8   dist_km            120000 non-null  float64\n",
            "dtypes: float64(6), int64(2), object(1)\n",
            "memory usage: 8.2+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5df7eBiLGaWe"
      },
      "source": [
        "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime']) #converting our pickup_datetime column into an actual date time rather than a string"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpYopKgcG1DI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a55df61-a346-4f78-fce6-c632c5ed327d"
      },
      "source": [
        "df.info() #notice how it's a datetime object now"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 120000 entries, 0 to 119999\n",
            "Data columns (total 9 columns):\n",
            " #   Column             Non-Null Count   Dtype              \n",
            "---  ------             --------------   -----              \n",
            " 0   pickup_datetime    120000 non-null  datetime64[ns, UTC]\n",
            " 1   fare_amount        120000 non-null  float64            \n",
            " 2   fare_class         120000 non-null  int64              \n",
            " 3   pickup_longitude   120000 non-null  float64            \n",
            " 4   pickup_latitude    120000 non-null  float64            \n",
            " 5   dropoff_longitude  120000 non-null  float64            \n",
            " 6   dropoff_latitude   120000 non-null  float64            \n",
            " 7   passenger_count    120000 non-null  int64              \n",
            " 8   dist_km            120000 non-null  float64            \n",
            "dtypes: datetime64[ns, UTC](1), float64(6), int64(2)\n",
            "memory usage: 8.2 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyKrhd0pHoGZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "9bcb8a0b-af5a-4312-ca67-de0219ae4cd0"
      },
      "source": [
        "df['EDTdate'] = df['pickup_datetime'] - pd.Timedelta(hours = 4) #converting our date time from UTC to EDT\n",
        "df.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>fare_class</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>dist_km</th>\n",
              "      <th>EDTdate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010-04-19 08:17:56+00:00</td>\n",
              "      <td>6.5</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.992365</td>\n",
              "      <td>40.730521</td>\n",
              "      <td>-73.975499</td>\n",
              "      <td>40.744746</td>\n",
              "      <td>1</td>\n",
              "      <td>2.126312</td>\n",
              "      <td>2010-04-19 04:17:56+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-04-17 15:43:53+00:00</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.990078</td>\n",
              "      <td>40.740558</td>\n",
              "      <td>-73.974232</td>\n",
              "      <td>40.744114</td>\n",
              "      <td>1</td>\n",
              "      <td>1.392307</td>\n",
              "      <td>2010-04-17 11:43:53+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2010-04-17 11:23:26+00:00</td>\n",
              "      <td>10.1</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.994149</td>\n",
              "      <td>40.751118</td>\n",
              "      <td>-73.960064</td>\n",
              "      <td>40.766235</td>\n",
              "      <td>2</td>\n",
              "      <td>3.326763</td>\n",
              "      <td>2010-04-17 07:23:26+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2010-04-11 21:25:03+00:00</td>\n",
              "      <td>8.9</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.990485</td>\n",
              "      <td>40.756422</td>\n",
              "      <td>-73.971205</td>\n",
              "      <td>40.748192</td>\n",
              "      <td>1</td>\n",
              "      <td>1.864129</td>\n",
              "      <td>2010-04-11 17:25:03+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-04-17 02:19:01+00:00</td>\n",
              "      <td>19.7</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.990976</td>\n",
              "      <td>40.734202</td>\n",
              "      <td>-73.905956</td>\n",
              "      <td>40.743115</td>\n",
              "      <td>1</td>\n",
              "      <td>7.231321</td>\n",
              "      <td>2010-04-16 22:19:01+00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            pickup_datetime  fare_amount  ...   dist_km                   EDTdate\n",
              "0 2010-04-19 08:17:56+00:00          6.5  ...  2.126312 2010-04-19 04:17:56+00:00\n",
              "1 2010-04-17 15:43:53+00:00          6.9  ...  1.392307 2010-04-17 11:43:53+00:00\n",
              "2 2010-04-17 11:23:26+00:00         10.1  ...  3.326763 2010-04-17 07:23:26+00:00\n",
              "3 2010-04-11 21:25:03+00:00          8.9  ...  1.864129 2010-04-11 17:25:03+00:00\n",
              "4 2010-04-17 02:19:01+00:00         19.7  ...  7.231321 2010-04-16 22:19:01+00:00\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEzWpO_4Ift9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "6f44d110-e6b5-4ddf-f217-fd3a0a9579e5"
      },
      "source": [
        "#so a neural network isn't going to understand what a date time object is when being fed into it. So we need to perform feature engineering to grab specific information from our date time object that can be fed into our NN.\n",
        "#Now that we're converted our pickup_datetime into a datetime object, it can be broken up into components, such as hour, minute, day, etc\n",
        "df['Hour'] = df['EDTdate'].dt.hour #making a new column to get the hour\n",
        "df['AMorPM'] = np.where( df['Hour'] < 12, 'am', 'pm') #if hour is less than 12, then it's AM, else it's PM\n",
        "df['Weekday'] = df['EDTdate'].dt.strftime(\"%a\") #getting the date of the week. Can also use .dt.dayofweek\n",
        "df.head() #So we feature engineered 3 more columns with specific information about the date time"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>fare_class</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>dist_km</th>\n",
              "      <th>EDTdate</th>\n",
              "      <th>Hour</th>\n",
              "      <th>AMorPM</th>\n",
              "      <th>Weekday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010-04-19 08:17:56+00:00</td>\n",
              "      <td>6.5</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.992365</td>\n",
              "      <td>40.730521</td>\n",
              "      <td>-73.975499</td>\n",
              "      <td>40.744746</td>\n",
              "      <td>1</td>\n",
              "      <td>2.126312</td>\n",
              "      <td>2010-04-19 04:17:56+00:00</td>\n",
              "      <td>4</td>\n",
              "      <td>am</td>\n",
              "      <td>Mon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-04-17 15:43:53+00:00</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.990078</td>\n",
              "      <td>40.740558</td>\n",
              "      <td>-73.974232</td>\n",
              "      <td>40.744114</td>\n",
              "      <td>1</td>\n",
              "      <td>1.392307</td>\n",
              "      <td>2010-04-17 11:43:53+00:00</td>\n",
              "      <td>11</td>\n",
              "      <td>am</td>\n",
              "      <td>Sat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2010-04-17 11:23:26+00:00</td>\n",
              "      <td>10.1</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.994149</td>\n",
              "      <td>40.751118</td>\n",
              "      <td>-73.960064</td>\n",
              "      <td>40.766235</td>\n",
              "      <td>2</td>\n",
              "      <td>3.326763</td>\n",
              "      <td>2010-04-17 07:23:26+00:00</td>\n",
              "      <td>7</td>\n",
              "      <td>am</td>\n",
              "      <td>Sat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2010-04-11 21:25:03+00:00</td>\n",
              "      <td>8.9</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.990485</td>\n",
              "      <td>40.756422</td>\n",
              "      <td>-73.971205</td>\n",
              "      <td>40.748192</td>\n",
              "      <td>1</td>\n",
              "      <td>1.864129</td>\n",
              "      <td>2010-04-11 17:25:03+00:00</td>\n",
              "      <td>17</td>\n",
              "      <td>pm</td>\n",
              "      <td>Sun</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-04-17 02:19:01+00:00</td>\n",
              "      <td>19.7</td>\n",
              "      <td>1</td>\n",
              "      <td>-73.990976</td>\n",
              "      <td>40.734202</td>\n",
              "      <td>-73.905956</td>\n",
              "      <td>40.743115</td>\n",
              "      <td>1</td>\n",
              "      <td>7.231321</td>\n",
              "      <td>2010-04-16 22:19:01+00:00</td>\n",
              "      <td>22</td>\n",
              "      <td>pm</td>\n",
              "      <td>Fri</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            pickup_datetime  fare_amount  fare_class  ...  Hour  AMorPM  Weekday\n",
              "0 2010-04-19 08:17:56+00:00          6.5           0  ...     4      am      Mon\n",
              "1 2010-04-17 15:43:53+00:00          6.9           0  ...    11      am      Sat\n",
              "2 2010-04-17 11:23:26+00:00         10.1           1  ...     7      am      Sat\n",
              "3 2010-04-11 21:25:03+00:00          8.9           0  ...    17      pm      Sun\n",
              "4 2010-04-17 02:19:01+00:00         19.7           1  ...    22      pm      Fri\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUHol08LK4jc"
      },
      "source": [
        "# **Part 2**\n",
        "Separating Categorical Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LMYlT74J_Fp"
      },
      "source": [
        "categorical_col = ['Hour', 'AMorPM', 'Weekday'] #categorical columns\n",
        "continuous_col = ['pickup_longitude','pickup_latitude', 'dropoff_longitude', 'dropoff_latitude','passenger_count', 'dist_km'] #continuous columns\n",
        "y_col = ['fare_amount'] #this will be the answer that we want to be the output of our NN"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_3djIz3Lo5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a5804d-913a-47a5-9383-8862ff6144d0"
      },
      "source": [
        "df.dtypes #notice weekday and AMorPM are objects, but we want to convert them into categoires"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pickup_datetime      datetime64[ns, UTC]\n",
              "fare_amount                      float64\n",
              "fare_class                         int64\n",
              "pickup_longitude                 float64\n",
              "pickup_latitude                  float64\n",
              "dropoff_longitude                float64\n",
              "dropoff_latitude                 float64\n",
              "passenger_count                    int64\n",
              "dist_km                          float64\n",
              "EDTdate              datetime64[ns, UTC]\n",
              "Hour                               int64\n",
              "AMorPM                            object\n",
              "Weekday                           object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJAmwJt3MM1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49920cb6-0079-40ed-a237-228015d81981"
      },
      "source": [
        "for cat in categorical_col:\n",
        "  df[cat] = df[cat].astype('category')\n",
        "\n",
        "df.dtypes #notice they are categories now"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pickup_datetime      datetime64[ns, UTC]\n",
              "fare_amount                      float64\n",
              "fare_class                         int64\n",
              "pickup_longitude                 float64\n",
              "pickup_latitude                  float64\n",
              "dropoff_longitude                float64\n",
              "dropoff_latitude                 float64\n",
              "passenger_count                    int64\n",
              "dist_km                          float64\n",
              "EDTdate              datetime64[ns, UTC]\n",
              "Hour                            category\n",
              "AMorPM                          category\n",
              "Weekday                         category\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtjtLBwrMm61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c931710c-8a25-4217-93a1-a7013d820960"
      },
      "source": [
        "df['Weekday'].head() #notice the weekday column is has 7 categories\r\n",
        "#similarly, AMorPM would have 2 categories, Hour would have 24 categories, etc"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Mon\n",
              "1    Sat\n",
              "2    Sat\n",
              "3    Sun\n",
              "4    Fri\n",
              "Name: Weekday, dtype: category\n",
              "Categories (7, object): ['Fri', 'Mon', 'Sat', 'Sun', 'Thu', 'Tue', 'Wed']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gny98utYM59_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "008d3cd6-f5cd-444b-90f2-7d19b41cd863"
      },
      "source": [
        "print(df['Weekday'].cat.codes.values) #so we're given a code for each day of the weekend from 0 to 6\r\n",
        "print(df['AMorPM'].cat.codes.values) #so we're given a code for AM = 0, PM = 1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 2 ... 3 5 2]\n",
            "[0 0 0 ... 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHNrq5cCNROr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61256dd7-b2ee-4b82-9af6-4799ec65cff0"
      },
      "source": [
        "hr = df['Hour'].cat.codes.values #extracting the code for each hour\n",
        "ampm = df['AMorPM'].cat.codes.values  #extracting the code for AM and PM\n",
        "wkdy = df['Weekday'].cat.codes.values  #extracting the code for each day of the week\n",
        "\n",
        "cats = np.stack([hr, ampm, wkdy], axis = 1) #so we're stacking the arrays together along the y-axis to make a row for each\n",
        "cats #first entry is 4 AM on Monday\n",
        "\n",
        "#could've done this all in one line using:\n",
        "#cats = np.stack([df[col].cat.codes.values for col in categorical_col], 1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4,  0,  1],\n",
              "       [11,  0,  2],\n",
              "       [ 7,  0,  2],\n",
              "       ...,\n",
              "       [14,  1,  3],\n",
              "       [ 4,  0,  5],\n",
              "       [12,  1,  2]], dtype=int8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8ziNMExONQn"
      },
      "source": [
        "cats = torch.tensor(cats, dtype = torch.int64) #converting our categories into a PyTorch Tensor to feed into our model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agjt9BIIPgOH"
      },
      "source": [
        "Separating Continuous Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcZf6744Pfe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4362295b-53b8-4fbb-8256-f6f60ddc19a5"
      },
      "source": [
        "conts = np.stack([df[col].values for col in cont_cols], axis = 1) #doing what we did above to categorical values, except in one line for continuous\n",
        "conts "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-73.992365  ,  40.730521  , -73.975499  ,  40.744746  ,\n",
              "          1.        ,   2.12631159],\n",
              "       [-73.990078  ,  40.740558  , -73.974232  ,  40.744114  ,\n",
              "          1.        ,   1.39230687],\n",
              "       [-73.994149  ,  40.751118  , -73.960064  ,  40.766235  ,\n",
              "          2.        ,   3.32676344],\n",
              "       ...,\n",
              "       [-73.988574  ,  40.749772  , -74.011541  ,  40.707799  ,\n",
              "          3.        ,   5.05252282],\n",
              "       [-74.004449  ,  40.724529  , -73.992697  ,  40.730765  ,\n",
              "          1.        ,   1.20892296],\n",
              "       [-73.955415  ,  40.77192   , -73.967623  ,  40.763015  ,\n",
              "          3.        ,   1.42739869]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7Adc9GJQK7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d11dee-a951-44f9-ece8-bd8d90b14f9c"
      },
      "source": [
        "conts = torch.tensor(conts, dtype=torch.float) #converting our continous variables into a PyTorch Tensor\n",
        "conts"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-73.9924,  40.7305, -73.9755,  40.7447,   1.0000,   2.1263],\n",
              "        [-73.9901,  40.7406, -73.9742,  40.7441,   1.0000,   1.3923],\n",
              "        [-73.9941,  40.7511, -73.9601,  40.7662,   2.0000,   3.3268],\n",
              "        ...,\n",
              "        [-73.9886,  40.7498, -74.0115,  40.7078,   3.0000,   5.0525],\n",
              "        [-74.0044,  40.7245, -73.9927,  40.7308,   1.0000,   1.2089],\n",
              "        [-73.9554,  40.7719, -73.9676,  40.7630,   3.0000,   1.4274]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiFQjEH3QV3a"
      },
      "source": [
        "Separating The Label (Answer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_08vX-XQT6X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa03c846-0079-46a5-efcf-70c5567d5a58"
      },
      "source": [
        "y = torch.tensor(df[y_col].values, dtype=torch.float)\n",
        "y"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6.5000],\n",
              "        [ 6.9000],\n",
              "        [10.1000],\n",
              "        ...,\n",
              "        [12.5000],\n",
              "        [ 4.9000],\n",
              "        [ 5.3000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn5T1IP4Qomu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eb973d6-f4e0-485f-e890-b849e5cb8dbb"
      },
      "source": [
        "print('cats.shape =',cats.shape)\n",
        "print('conts.shape =',conts.shape)\n",
        "print('y.shape =',y.shape)\n",
        "#shapes are as expected with 3 columns in categorical, 6 columns in continuous, and 1 column as the label"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cats.shape = torch.Size([120000, 3])\n",
            "conts.shape = torch.Size([120000, 6])\n",
            "y.shape = torch.Size([120000, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVuTBWmTRqxv"
      },
      "source": [
        "Embedding our Categories (One Hot Encoding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDD58WB-RIMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3945497c-28ca-4143-deb2-bbbbc772aff4"
      },
      "source": [
        "categorical_sizes = [len(df[col].cat.categories) for col in categorical_col] #category sizes\n",
        "\n",
        "categorical_sizes #shows how many categories we have in each column, so our case is 24 hours in a day, 2 (AM or PM), and 7 days in a week"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[24, 2, 7]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wyta0DW6SOKS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb085229-bdd0-4bbe-ba2b-26cc605a7348"
      },
      "source": [
        "emb_sizes = [(size, min(50, (size+1)//2)) for size in categorical_sizes] #so for size (24 ,2 ,7) in cat_szs, embed 50 or take the size and divide by 2\n",
        "#the +1 is incase we only have 2 categories and // is to make sure the output is an integer\n",
        "emb_sizes #none of the sizes exceeded 50, so it just made the embedding size the size divided by 2. \n",
        "#so here, our number of categories are 24, 2, 7 and our embedding sizes are 12, 1, 4\n",
        "\n",
        "#So the embedding provides a more compact (lower dimensional) representation of a set of input variables that have some correlations. It's useful because it reduces the number of parameters overall. \n",
        "#So technically we could've just used all 24 hours + 2 am or pm + 7 days of the week as inputs into our NN, but we used embedding to reduce the parameters and make calculations faster\n",
        "#An embedding layer simply maps/projects some N dimensional tensor into another number of dimensions, the number of dimensions to output to are often arbitrary, but you can have some logic to it, like embedding 7 days a week to 2 dimensions may end up mapping closely to weekday vs weekends (although there is no guarantee of this)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(24, 12), (2, 1), (7, 4)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wSwBB0jC6RC"
      },
      "source": [
        "# **Part 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a27YDHSlKmXZ"
      },
      "source": [
        "Creating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldYsdyJEJAHX"
      },
      "source": [
        "#Inheriting from nn.Module to use it's features\n",
        "class Model(nn.Module):\n",
        "  def __init__(self, emb_sizes, n_continuous, out_size, n_layers, p=0.5):\n",
        "    #so our parameters are embedding size, number of continous features, output size, number of layers, and our dropout probability defaulted to 50%\n",
        "    #This determins how many layers are there in our NN\n",
        "    #Input Layer (4 features from Iris dataset)--> Hidden Layer 1 --> Hidden Later 2 --> Output (3 classes)\n",
        "\n",
        "    super().__init__() #inheriting the features from nn.Module\n",
        "    self.embeds = nn.ModuleList([nn.Embedding(num_embeddings = ni, embedding_dim = nf) for ni,nf in emb_sizes]) #so now our ModuleList is filled with the Embedding layers (one for hour, one of ampm, and one for weekday)\n",
        "    self.emb_drop = nn.Dropout(p) #adding a dropout layer\n",
        "    self.bn_cont = nn.BatchNorm1d(n_continuous) #normalizing the continous data so it falls within the same order of magnitude range\n",
        "\n",
        "    layerlist = [] #initializing layerlist so we can store our layers\n",
        "    n_emb = sum([nf for ni,nf in emb_sizes]) #calculating the number of embeddings, which is basically 12 + 1 + 4\n",
        "    n_in = n_emb + n_continuous #so the total number of inputs will be the number of embeddings plus the number of continous features\n",
        "\n",
        "    for i in n_layers:\n",
        "      layerlist.append(nn.Linear(n_in, i)) #create a layer connecting n_in and i, where i is the number of neurons. Basically this makes a fully connected layer. \n",
        "      layerlist.append(nn.ReLU(inplace = True)) #adding the activation function to the layer\n",
        "      layerlist.append(nn.BatchNorm1d(i)) #adding the normalization of the continuous values to the list\n",
        "      layerlist.append(nn.Dropout(p)) #adding the dropout to the layer\n",
        "      n_in = i #equating n_in with the number of neurons i, so that the next fully connected layer can start with i - 1 neurons.\n",
        "\n",
        "    #So layers is going to look like this: layers = [256, 128, 64] meaning you want 256 neurons in first layer, then 128 neurons in second layer, then 64. This allows us play around with the values and makes it flexible. \n",
        "    layerlist.append(nn.Linear(n_layers[-1], out_size)) #so making the last layer be connected by the last layer and output size\n",
        "    self.layers = nn.Sequential(*layerlist) #using sequential to combine the layers from the layerlist so that it can be ordered properly as a NN\n",
        "\n",
        "  def forward(self, x_categorical, x_continuous):\n",
        "    #So this determines the forward propagation with the activation functions we want to use\n",
        "\n",
        "    # Now we are utilizing the embeddings to pass into our forward method\n",
        "    embeddings = [] \n",
        "\n",
        "    for i,e in enumerate(self.embeds):\n",
        "      embeddings.append(e(x_categorical[:,i])) #so we're adding to our embeddingz list for each row and column i in our categorical inputs, x_cat\n",
        "\n",
        "    x = torch.cat(tensors = embeddings, dim = 1) #so we're concatenating embeddings along dimension 1, meaning all the values we got above for each tensor will be within a single row (i.e. 12(hour) + 1(amorpm) + 4(weekday) all in one row within a single tensor)\n",
        "    x = self.emb_drop(x) #adding the drop out layer to our embedded categorical data\n",
        "\n",
        "    x_cont = self.bn_cont(x_continuous) #normalizing our continuous data\n",
        "    x = torch.cat(tensors = [x, x_continuous], dim = 1) #concatenating our categorical and continuous data to be in a single tensor row\n",
        "    x = self.layers(x) #passing our continuous and categorical data into our NN, which was made above with Sequential\n",
        "\n",
        "    return x"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUlOYxetCvRX"
      },
      "source": [
        "# **Part 4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJJbQ5KhS1WJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c382d95a-0695-421d-a79e-16ab4a1bc07f"
      },
      "source": [
        "model = Model(emb_sizes,  n_continuous = conts.shape[1], out_size = 1, n_layers = [256, 128, 64], p=0.5) #conts.shape[1] is the number of columns in the continuous tensor, output size is 1, and hidden layers will have 200 then 100 neurons\n",
        "model"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (embeds): ModuleList(\n",
              "    (0): Embedding(24, 12)\n",
              "    (1): Embedding(2, 1)\n",
              "    (2): Embedding(7, 4)\n",
              "  )\n",
              "  (emb_drop): Dropout(p=0.5, inplace=False)\n",
              "  (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=23, out_features=256, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): Dropout(p=0.5, inplace=False)\n",
              "    (8): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): Dropout(p=0.5, inplace=False)\n",
              "    (12): Linear(in_features=64, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJhFUJ3fDk-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "560a7c1e-2d15-497c-c6ab-09c70e197032"
      },
      "source": [
        "criterion = nn.MSELoss() #will convert this later to RMS so that our units are squared\n",
        "\n",
        "optimizer = torch.optim.Adam(params = model.parameters(), lr = 0.001) #model parameters are just the fully connected layers and we are using Adam optimizer to optimize them\n",
        "model.parameters #can see the parameters are just the fully connected layers and we are optimizing them"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of Model(\n",
              "  (embeds): ModuleList(\n",
              "    (0): Embedding(24, 12)\n",
              "    (1): Embedding(2, 1)\n",
              "    (2): Embedding(7, 4)\n",
              "  )\n",
              "  (emb_drop): Dropout(p=0.5, inplace=False)\n",
              "  (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=23, out_features=256, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): Dropout(p=0.5, inplace=False)\n",
              "    (8): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): Dropout(p=0.5, inplace=False)\n",
              "    (12): Linear(in_features=64, out_features=1, bias=True)\n",
              "  )\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lBVqiF_Et2E"
      },
      "source": [
        "batch_size = 120000 #just going to feed in all the data\n",
        "test_size = int(batch_size*0.25) #making our test size 1/3 the batch size"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1j5OP9mFFt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f51136eb-7012-417f-b294-f0cb167b4ed6"
      },
      "source": [
        "#Our data was shuffled already when we got it, which is why we can do the following. If it wasn't shuffled already, then we'd need to shuffle first before assigning the train test split\n",
        "\n",
        "categorical_train = cats[:batch_size - test_size] #so take from index 0 to 24000\n",
        "categorical_test = cats[batch_size - test_size: batch_size] #take from index 24000 to 30000\n",
        "continuous_train = conts[:batch_size - test_size]\n",
        "continuous_test = conts[batch_size - test_size: batch_size]\n",
        "y_train = y[:batch_size - test_size]\n",
        "y_test = y[batch_size - test_size: batch_size]\n",
        "\n",
        "print(len(categorical_train))\n",
        "print(len(categorical_test))\n",
        "print(len(continuous_train))\n",
        "print(len(continuous_test))\n",
        "print(len(y_train))\n",
        "print(len(y_test))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "90000\n",
            "30000\n",
            "90000\n",
            "30000\n",
            "90000\n",
            "30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6SCyakqHI8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a87ab5-f8f2-4fda-9fac-23b462360f16"
      },
      "source": [
        "import time #Gonna try to keep track of how long it takes to train our model\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "epochs = 450\n",
        "\n",
        "losses = []\n",
        "\n",
        "#This for loop trains our NN\n",
        "for i in range(epochs):\n",
        "  #Forward propagation through our ANN using training data\n",
        "  y_pred = model.forward(categorical_train, continuous_train)\n",
        "\n",
        "  #Calculating loss/error\n",
        "  loss = torch.sqrt(criterion(y_pred, y_train)) #turning our MSE into RMSE so our units don't get squared\n",
        "  losses.append(loss)\n",
        "\n",
        "  print(f'epoch {i} and loss is: {loss} and time: {(time.time() - start_time)/60}')\n",
        "\n",
        "  #Backpropagation\n",
        "  optimizer.zero_grad() #resetting the gradient on the optimizer so it doesn't accumulate\n",
        "  loss.backward() #doing backpropagation off the loss function\n",
        "  optimizer.step() #using the optimizer for the back propagation\n",
        "\n",
        "print(f'Training took {(time.time() - start_time)/60} minutes')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 and loss is: 12.438814163208008 and time: 0.03742837905883789\n",
            "epoch 1 and loss is: 12.27503490447998 and time: 0.13643746376037597\n",
            "epoch 2 and loss is: 12.119855880737305 and time: 0.23397483030954996\n",
            "epoch 3 and loss is: 11.99235725402832 and time: 0.33265262842178345\n",
            "epoch 4 and loss is: 11.879670143127441 and time: 0.4311389724413554\n",
            "epoch 5 and loss is: 11.780956268310547 and time: 0.5300045291582743\n",
            "epoch 6 and loss is: 11.684331893920898 and time: 0.6268419067064921\n",
            "epoch 7 and loss is: 11.60068416595459 and time: 0.7245704849561055\n",
            "epoch 8 and loss is: 11.532145500183105 and time: 0.8214169383049011\n",
            "epoch 9 and loss is: 11.467344284057617 and time: 0.9189644614855449\n",
            "epoch 10 and loss is: 11.403987884521484 and time: 1.0166694362958273\n",
            "epoch 11 and loss is: 11.357162475585938 and time: 1.1139453967412314\n",
            "epoch 12 and loss is: 11.299732208251953 and time: 1.211779816945394\n",
            "epoch 13 and loss is: 11.25107479095459 and time: 1.3086176911989849\n",
            "epoch 14 and loss is: 11.219152450561523 and time: 1.4058364391326905\n",
            "epoch 15 and loss is: 11.17641544342041 and time: 1.503272823492686\n",
            "epoch 16 and loss is: 11.138575553894043 and time: 1.6006972789764404\n",
            "epoch 17 and loss is: 11.102569580078125 and time: 1.6975334485371907\n",
            "epoch 18 and loss is: 11.071807861328125 and time: 1.7957372546195984\n",
            "epoch 19 and loss is: 11.041006088256836 and time: 1.8922977526982625\n",
            "epoch 20 and loss is: 11.01112174987793 and time: 1.9893776814142863\n",
            "epoch 21 and loss is: 10.978708267211914 and time: 2.086137318611145\n",
            "epoch 22 and loss is: 10.958545684814453 and time: 2.183519160747528\n",
            "epoch 23 and loss is: 10.924439430236816 and time: 2.2809654553731282\n",
            "epoch 24 and loss is: 10.89842700958252 and time: 2.3795920332272846\n",
            "epoch 25 and loss is: 10.872801780700684 and time: 2.4786078095436097\n",
            "epoch 26 and loss is: 10.851577758789062 and time: 2.5807061076164244\n",
            "epoch 27 and loss is: 10.82399845123291 and time: 2.6787715752919516\n",
            "epoch 28 and loss is: 10.80481243133545 and time: 2.7768100023269655\n",
            "epoch 29 and loss is: 10.763092041015625 and time: 2.8743141651153565\n",
            "epoch 30 and loss is: 10.74685001373291 and time: 2.9718623638153074\n",
            "epoch 31 and loss is: 10.710697174072266 and time: 3.0692232211430865\n",
            "epoch 32 and loss is: 10.702536582946777 and time: 3.167811175187429\n",
            "epoch 33 and loss is: 10.665677070617676 and time: 3.265751767158508\n",
            "epoch 34 and loss is: 10.653553009033203 and time: 3.3651060382525126\n",
            "epoch 35 and loss is: 10.625883102416992 and time: 3.4642323652903237\n",
            "epoch 36 and loss is: 10.608449935913086 and time: 3.562078348795573\n",
            "epoch 37 and loss is: 10.585683822631836 and time: 3.6598132332166036\n",
            "epoch 38 and loss is: 10.567122459411621 and time: 3.7594762365023295\n",
            "epoch 39 and loss is: 10.534350395202637 and time: 3.857473119099935\n",
            "epoch 40 and loss is: 10.511080741882324 and time: 3.9547950585683185\n",
            "epoch 41 and loss is: 10.502718925476074 and time: 4.052134048938751\n",
            "epoch 42 and loss is: 10.481606483459473 and time: 4.149028050899505\n",
            "epoch 43 and loss is: 10.454768180847168 and time: 4.246306375662486\n",
            "epoch 44 and loss is: 10.436553955078125 and time: 4.344472714265188\n",
            "epoch 45 and loss is: 10.415307998657227 and time: 4.441636502742767\n",
            "epoch 46 and loss is: 10.396084785461426 and time: 4.53887308438619\n",
            "epoch 47 and loss is: 10.374092102050781 and time: 4.635835206508636\n",
            "epoch 48 and loss is: 10.344122886657715 and time: 4.7327734192212425\n",
            "epoch 49 and loss is: 10.336387634277344 and time: 4.829284222920736\n",
            "epoch 50 and loss is: 10.31799030303955 and time: 4.926522477467855\n",
            "epoch 51 and loss is: 10.294751167297363 and time: 5.023573688666025\n",
            "epoch 52 and loss is: 10.270491600036621 and time: 5.120519717534383\n",
            "epoch 53 and loss is: 10.262737274169922 and time: 5.217902414004008\n",
            "epoch 54 and loss is: 10.243367195129395 and time: 5.316737806797027\n",
            "epoch 55 and loss is: 10.226179122924805 and time: 5.414408532778422\n",
            "epoch 56 and loss is: 10.21173095703125 and time: 5.513893695672353\n",
            "epoch 57 and loss is: 10.189823150634766 and time: 5.610479978720347\n",
            "epoch 58 and loss is: 10.177277565002441 and time: 5.708404064178467\n",
            "epoch 59 and loss is: 10.157312393188477 and time: 5.8056120077768965\n",
            "epoch 60 and loss is: 10.15045166015625 and time: 5.902664220333099\n",
            "epoch 61 and loss is: 10.124558448791504 and time: 6.000035635630289\n",
            "epoch 62 and loss is: 10.112135887145996 and time: 6.096030310789744\n",
            "epoch 63 and loss is: 10.100460052490234 and time: 6.19353023370107\n",
            "epoch 64 and loss is: 10.088210105895996 and time: 6.2900464057922365\n",
            "epoch 65 and loss is: 10.071014404296875 and time: 6.386467134952545\n",
            "epoch 66 and loss is: 10.055621147155762 and time: 6.483589971065522\n",
            "epoch 67 and loss is: 10.04386043548584 and time: 6.58002370595932\n",
            "epoch 68 and loss is: 10.026036262512207 and time: 6.677098417282105\n",
            "epoch 69 and loss is: 10.007322311401367 and time: 6.774327301979065\n",
            "epoch 70 and loss is: 9.9956693649292 and time: 6.871151085694631\n",
            "epoch 71 and loss is: 9.980255126953125 and time: 6.968254868189494\n",
            "epoch 72 and loss is: 9.957903861999512 and time: 7.064929743607839\n",
            "epoch 73 and loss is: 9.95413875579834 and time: 7.1616472999254865\n",
            "epoch 74 and loss is: 9.933696746826172 and time: 7.258813242117564\n",
            "epoch 75 and loss is: 9.920433044433594 and time: 7.355935462315878\n",
            "epoch 76 and loss is: 9.910662651062012 and time: 7.453330099582672\n",
            "epoch 77 and loss is: 9.895171165466309 and time: 7.550347439448038\n",
            "epoch 78 and loss is: 9.88782787322998 and time: 7.6470029274622595\n",
            "epoch 79 and loss is: 9.872064590454102 and time: 7.74425280491511\n",
            "epoch 80 and loss is: 9.850954055786133 and time: 7.841519530614217\n",
            "epoch 81 and loss is: 9.841511726379395 and time: 7.939311460653941\n",
            "epoch 82 and loss is: 9.820596694946289 and time: 8.036181382338206\n",
            "epoch 83 and loss is: 9.804837226867676 and time: 8.132803277174633\n",
            "epoch 84 and loss is: 9.801405906677246 and time: 8.229656700293223\n",
            "epoch 85 and loss is: 9.776890754699707 and time: 8.326137200991313\n",
            "epoch 86 and loss is: 9.775720596313477 and time: 8.42292942206065\n",
            "epoch 87 and loss is: 9.762327194213867 and time: 8.521775396664937\n",
            "epoch 88 and loss is: 9.735261917114258 and time: 8.619815842310588\n",
            "epoch 89 and loss is: 9.727697372436523 and time: 8.716574720541637\n",
            "epoch 90 and loss is: 9.71262264251709 and time: 8.813062957922618\n",
            "epoch 91 and loss is: 9.707290649414062 and time: 8.912260647614797\n",
            "epoch 92 and loss is: 9.685174942016602 and time: 9.009521353244782\n",
            "epoch 93 and loss is: 9.666444778442383 and time: 9.106281169255574\n",
            "epoch 94 and loss is: 9.655473709106445 and time: 9.203591759999593\n",
            "epoch 95 and loss is: 9.63618278503418 and time: 9.300079492727916\n",
            "epoch 96 and loss is: 9.615588188171387 and time: 9.397001496950786\n",
            "epoch 97 and loss is: 9.610184669494629 and time: 9.494251044591268\n",
            "epoch 98 and loss is: 9.595279693603516 and time: 9.591326145331065\n",
            "epoch 99 and loss is: 9.571651458740234 and time: 9.687941785653432\n",
            "epoch 100 and loss is: 9.567658424377441 and time: 9.784595255057017\n",
            "epoch 101 and loss is: 9.547789573669434 and time: 9.881583333015442\n",
            "epoch 102 and loss is: 9.541024208068848 and time: 9.978168682257335\n",
            "epoch 103 and loss is: 9.529702186584473 and time: 10.074490602811178\n",
            "epoch 104 and loss is: 9.506697654724121 and time: 10.170956556002299\n",
            "epoch 105 and loss is: 9.484038352966309 and time: 10.267852040131887\n",
            "epoch 106 and loss is: 9.470183372497559 and time: 10.364476692676543\n",
            "epoch 107 and loss is: 9.458906173706055 and time: 10.46222311258316\n",
            "epoch 108 and loss is: 9.428635597229004 and time: 10.559412908554076\n",
            "epoch 109 and loss is: 9.425823211669922 and time: 10.656518618265787\n",
            "epoch 110 and loss is: 9.399083137512207 and time: 10.753644585609436\n",
            "epoch 111 and loss is: 9.403200149536133 and time: 10.851369496186575\n",
            "epoch 112 and loss is: 9.373409271240234 and time: 10.950102361043294\n",
            "epoch 113 and loss is: 9.355230331420898 and time: 11.048282365004221\n",
            "epoch 114 and loss is: 9.343876838684082 and time: 11.146537065505981\n",
            "epoch 115 and loss is: 9.318277359008789 and time: 11.245599794387818\n",
            "epoch 116 and loss is: 9.305074691772461 and time: 11.342680903275808\n",
            "epoch 117 and loss is: 9.28870677947998 and time: 11.440588716665903\n",
            "epoch 118 and loss is: 9.271718978881836 and time: 11.538567026456198\n",
            "epoch 119 and loss is: 9.246516227722168 and time: 11.635934456189473\n",
            "epoch 120 and loss is: 9.244629859924316 and time: 11.732633829116821\n",
            "epoch 121 and loss is: 9.21961784362793 and time: 11.829862248897552\n",
            "epoch 122 and loss is: 9.200119972229004 and time: 11.927489038308462\n",
            "epoch 123 and loss is: 9.182544708251953 and time: 12.02440140247345\n",
            "epoch 124 and loss is: 9.17245864868164 and time: 12.121153942743938\n",
            "epoch 125 and loss is: 9.142724990844727 and time: 12.218787880738576\n",
            "epoch 126 and loss is: 9.12971019744873 and time: 12.317161651452382\n",
            "epoch 127 and loss is: 9.11758804321289 and time: 12.415022778511048\n",
            "epoch 128 and loss is: 9.094564437866211 and time: 12.51471410592397\n",
            "epoch 129 and loss is: 9.081636428833008 and time: 12.614066322644552\n",
            "epoch 130 and loss is: 9.06257152557373 and time: 12.714529530207317\n",
            "epoch 131 and loss is: 9.046832084655762 and time: 12.812467563152314\n",
            "epoch 132 and loss is: 9.023930549621582 and time: 12.910952218373616\n",
            "epoch 133 and loss is: 9.00290584564209 and time: 13.00902042388916\n",
            "epoch 134 and loss is: 8.978743553161621 and time: 13.106594653924306\n",
            "epoch 135 and loss is: 8.953612327575684 and time: 13.205548775196075\n",
            "epoch 136 and loss is: 8.952615737915039 and time: 13.303143246968586\n",
            "epoch 137 and loss is: 8.91555118560791 and time: 13.400700867176056\n",
            "epoch 138 and loss is: 8.898883819580078 and time: 13.498887729644775\n",
            "epoch 139 and loss is: 8.873666763305664 and time: 13.596236093839009\n",
            "epoch 140 and loss is: 8.844209671020508 and time: 13.69814639488856\n",
            "epoch 141 and loss is: 8.83224868774414 and time: 13.802160294850667\n",
            "epoch 142 and loss is: 8.820290565490723 and time: 13.905247521400451\n",
            "epoch 143 and loss is: 8.787312507629395 and time: 14.01143806775411\n",
            "epoch 144 and loss is: 8.77123737335205 and time: 14.109269086519877\n",
            "epoch 145 and loss is: 8.744214057922363 and time: 14.20655968983968\n",
            "epoch 146 and loss is: 8.722063064575195 and time: 14.303271198272705\n",
            "epoch 147 and loss is: 8.701849937438965 and time: 14.400378068288168\n",
            "epoch 148 and loss is: 8.674177169799805 and time: 14.49770458539327\n",
            "epoch 149 and loss is: 8.658346176147461 and time: 14.5946666320165\n",
            "epoch 150 and loss is: 8.635446548461914 and time: 14.691765944163004\n",
            "epoch 151 and loss is: 8.617417335510254 and time: 14.789696784814199\n",
            "epoch 152 and loss is: 8.596460342407227 and time: 14.887059613068898\n",
            "epoch 153 and loss is: 8.570874214172363 and time: 14.984863007068634\n",
            "epoch 154 and loss is: 8.529840469360352 and time: 15.082459545135498\n",
            "epoch 155 and loss is: 8.508068084716797 and time: 15.180247116088868\n",
            "epoch 156 and loss is: 8.482356071472168 and time: 15.279341173171996\n",
            "epoch 157 and loss is: 8.466829299926758 and time: 15.379400698343913\n",
            "epoch 158 and loss is: 8.441520690917969 and time: 15.479129799207051\n",
            "epoch 159 and loss is: 8.418930053710938 and time: 15.578283635775248\n",
            "epoch 160 and loss is: 8.388422012329102 and time: 15.675360596179962\n",
            "epoch 161 and loss is: 8.358829498291016 and time: 15.772261607646943\n",
            "epoch 162 and loss is: 8.332798957824707 and time: 15.869038152694703\n",
            "epoch 163 and loss is: 8.307886123657227 and time: 15.966613229115804\n",
            "epoch 164 and loss is: 8.285225868225098 and time: 16.06336763302485\n",
            "epoch 165 and loss is: 8.25465202331543 and time: 16.161900003751118\n",
            "epoch 166 and loss is: 8.223634719848633 and time: 16.259576312700908\n",
            "epoch 167 and loss is: 8.190775871276855 and time: 16.35671336253484\n",
            "epoch 168 and loss is: 8.173446655273438 and time: 16.454474703470865\n",
            "epoch 169 and loss is: 8.138605117797852 and time: 16.5518613020579\n",
            "epoch 170 and loss is: 8.105813026428223 and time: 16.648884069919585\n",
            "epoch 171 and loss is: 8.087892532348633 and time: 16.746855755647022\n",
            "epoch 172 and loss is: 8.067011833190918 and time: 16.845151952902476\n",
            "epoch 173 and loss is: 8.025195121765137 and time: 16.943369563420614\n",
            "epoch 174 and loss is: 7.998868465423584 and time: 17.041382392247517\n",
            "epoch 175 and loss is: 7.973475456237793 and time: 17.13962325255076\n",
            "epoch 176 and loss is: 7.935342311859131 and time: 17.23882131576538\n",
            "epoch 177 and loss is: 7.9093427658081055 and time: 17.339272538820904\n",
            "epoch 178 and loss is: 7.883907794952393 and time: 17.44191376765569\n",
            "epoch 179 and loss is: 7.856459140777588 and time: 17.544055664539336\n",
            "epoch 180 and loss is: 7.822063446044922 and time: 17.64514815012614\n",
            "epoch 181 and loss is: 7.785808563232422 and time: 17.74321221113205\n",
            "epoch 182 and loss is: 7.750538349151611 and time: 17.84097331762314\n",
            "epoch 183 and loss is: 7.725001811981201 and time: 17.939656428496043\n",
            "epoch 184 and loss is: 7.702753067016602 and time: 18.03695648908615\n",
            "epoch 185 and loss is: 7.675856590270996 and time: 18.13455257813136\n",
            "epoch 186 and loss is: 7.626718521118164 and time: 18.232574959595997\n",
            "epoch 187 and loss is: 7.591272354125977 and time: 18.329646754264832\n",
            "epoch 188 and loss is: 7.566251754760742 and time: 18.42743926048279\n",
            "epoch 189 and loss is: 7.53826904296875 and time: 18.525305326779684\n",
            "epoch 190 and loss is: 7.492985725402832 and time: 18.623221770922342\n",
            "epoch 191 and loss is: 7.472999095916748 and time: 18.7212344566981\n",
            "epoch 192 and loss is: 7.434663772583008 and time: 18.818778916200003\n",
            "epoch 193 and loss is: 7.414371967315674 and time: 18.91634883880615\n",
            "epoch 194 and loss is: 7.373553276062012 and time: 19.01487592061361\n",
            "epoch 195 and loss is: 7.332052230834961 and time: 19.118168822924297\n",
            "epoch 196 and loss is: 7.309009075164795 and time: 19.226388784249625\n",
            "epoch 197 and loss is: 7.27202033996582 and time: 19.325844013690947\n",
            "epoch 198 and loss is: 7.226309776306152 and time: 19.42399806578954\n",
            "epoch 199 and loss is: 7.211842060089111 and time: 19.52192529439926\n",
            "epoch 200 and loss is: 7.168050765991211 and time: 19.61911643743515\n",
            "epoch 201 and loss is: 7.135710716247559 and time: 19.71700845162074\n",
            "epoch 202 and loss is: 7.104037761688232 and time: 19.814444828033448\n",
            "epoch 203 and loss is: 7.067623615264893 and time: 19.91172225077947\n",
            "epoch 204 and loss is: 7.02595853805542 and time: 20.009396251042684\n",
            "epoch 205 and loss is: 6.994250774383545 and time: 20.106574153900148\n",
            "epoch 206 and loss is: 6.962046146392822 and time: 20.2036225994428\n",
            "epoch 207 and loss is: 6.9453206062316895 and time: 20.30071895122528\n",
            "epoch 208 and loss is: 6.892282009124756 and time: 20.39786199728648\n",
            "epoch 209 and loss is: 6.868114948272705 and time: 20.4963099638621\n",
            "epoch 210 and loss is: 6.829344272613525 and time: 20.59353661139806\n",
            "epoch 211 and loss is: 6.7847580909729 and time: 20.69069012403488\n",
            "epoch 212 and loss is: 6.774707794189453 and time: 20.787860453128815\n",
            "epoch 213 and loss is: 6.739912509918213 and time: 20.88510961929957\n",
            "epoch 214 and loss is: 6.689681529998779 and time: 20.983120715618135\n",
            "epoch 215 and loss is: 6.670669078826904 and time: 21.08061704238256\n",
            "epoch 216 and loss is: 6.614813804626465 and time: 21.178435599803926\n",
            "epoch 217 and loss is: 6.588010787963867 and time: 21.27620154619217\n",
            "epoch 218 and loss is: 6.571192264556885 and time: 21.37323338985443\n",
            "epoch 219 and loss is: 6.514587879180908 and time: 21.471106191476185\n",
            "epoch 220 and loss is: 6.475645542144775 and time: 21.56870381832123\n",
            "epoch 221 and loss is: 6.463528156280518 and time: 21.666232001781463\n",
            "epoch 222 and loss is: 6.411172389984131 and time: 21.764254148801168\n",
            "epoch 223 and loss is: 6.384248733520508 and time: 21.86154837210973\n",
            "epoch 224 and loss is: 6.3536505699157715 and time: 21.95954985221227\n",
            "epoch 225 and loss is: 6.313403129577637 and time: 22.056404383977256\n",
            "epoch 226 and loss is: 6.27356481552124 and time: 22.152630376815797\n",
            "epoch 227 and loss is: 6.229883193969727 and time: 22.250739069779716\n",
            "epoch 228 and loss is: 6.186138153076172 and time: 22.348087441921233\n",
            "epoch 229 and loss is: 6.162285327911377 and time: 22.44614940881729\n",
            "epoch 230 and loss is: 6.143311023712158 and time: 22.544397763411204\n",
            "epoch 231 and loss is: 6.108065128326416 and time: 22.64370710849762\n",
            "epoch 232 and loss is: 6.0685625076293945 and time: 22.74086303313573\n",
            "epoch 233 and loss is: 6.028711795806885 and time: 22.837752981980643\n",
            "epoch 234 and loss is: 5.987550735473633 and time: 22.93530543645223\n",
            "epoch 235 and loss is: 5.974626064300537 and time: 23.03347153266271\n",
            "epoch 236 and loss is: 5.923094272613525 and time: 23.130237193902335\n",
            "epoch 237 and loss is: 5.913185119628906 and time: 23.22814834912618\n",
            "epoch 238 and loss is: 5.872478485107422 and time: 23.32472284634908\n",
            "epoch 239 and loss is: 5.810368061065674 and time: 23.421918086210887\n",
            "epoch 240 and loss is: 5.780890464782715 and time: 23.5196501493454\n",
            "epoch 241 and loss is: 5.73469352722168 and time: 23.617925028006237\n",
            "epoch 242 and loss is: 5.738123416900635 and time: 23.71611720720927\n",
            "epoch 243 and loss is: 5.701466083526611 and time: 23.814343436559042\n",
            "epoch 244 and loss is: 5.649071216583252 and time: 23.913275063037872\n",
            "epoch 245 and loss is: 5.6262006759643555 and time: 24.0122771024704\n",
            "epoch 246 and loss is: 5.574151515960693 and time: 24.10948037703832\n",
            "epoch 247 and loss is: 5.547336101531982 and time: 24.20898084640503\n",
            "epoch 248 and loss is: 5.530097007751465 and time: 24.310704569021862\n",
            "epoch 249 and loss is: 5.487736701965332 and time: 24.40921540260315\n",
            "epoch 250 and loss is: 5.464797019958496 and time: 24.514395209153495\n",
            "epoch 251 and loss is: 5.433778762817383 and time: 24.61307772795359\n",
            "epoch 252 and loss is: 5.38666296005249 and time: 24.71120715936025\n",
            "epoch 253 and loss is: 5.356791973114014 and time: 24.809475473562877\n",
            "epoch 254 and loss is: 5.334801197052002 and time: 24.908122611045837\n",
            "epoch 255 and loss is: 5.300008773803711 and time: 25.007350103060404\n",
            "epoch 256 and loss is: 5.2763166427612305 and time: 25.107033133506775\n",
            "epoch 257 and loss is: 5.253459930419922 and time: 25.207231533527374\n",
            "epoch 258 and loss is: 5.215692043304443 and time: 25.305204804738363\n",
            "epoch 259 and loss is: 5.20161247253418 and time: 25.403902184963226\n",
            "epoch 260 and loss is: 5.158407688140869 and time: 25.504206017653146\n",
            "epoch 261 and loss is: 5.150742530822754 and time: 25.602919256687166\n",
            "epoch 262 and loss is: 5.1281418800354 and time: 25.70127144654592\n",
            "epoch 263 and loss is: 5.0576958656311035 and time: 25.799087262153627\n",
            "epoch 264 and loss is: 5.046137809753418 and time: 25.896857265631358\n",
            "epoch 265 and loss is: 5.029850482940674 and time: 25.995267868041992\n",
            "epoch 266 and loss is: 5.023545265197754 and time: 26.0947656750679\n",
            "epoch 267 and loss is: 4.970003128051758 and time: 26.19242335955302\n",
            "epoch 268 and loss is: 4.955086708068848 and time: 26.28978990316391\n",
            "epoch 269 and loss is: 4.905522346496582 and time: 26.388672550519306\n",
            "epoch 270 and loss is: 4.901658058166504 and time: 26.488234535853067\n",
            "epoch 271 and loss is: 4.885898113250732 and time: 26.587577060858408\n",
            "epoch 272 and loss is: 4.861443042755127 and time: 26.686887375513713\n",
            "epoch 273 and loss is: 4.81873083114624 and time: 26.785315557320914\n",
            "epoch 274 and loss is: 4.827673435211182 and time: 26.883338928222656\n",
            "epoch 275 and loss is: 4.784853935241699 and time: 26.982713508605958\n",
            "epoch 276 and loss is: 4.770055770874023 and time: 27.082498466968538\n",
            "epoch 277 and loss is: 4.766733646392822 and time: 27.181815048058827\n",
            "epoch 278 and loss is: 4.726988792419434 and time: 27.28086214462916\n",
            "epoch 279 and loss is: 4.680811882019043 and time: 27.380700743198396\n",
            "epoch 280 and loss is: 4.682249069213867 and time: 27.48342198530833\n",
            "epoch 281 and loss is: 4.675848960876465 and time: 27.585811336835224\n",
            "epoch 282 and loss is: 4.654464244842529 and time: 27.68820855220159\n",
            "epoch 283 and loss is: 4.612138748168945 and time: 27.788016605377198\n",
            "epoch 284 and loss is: 4.620527744293213 and time: 27.88707421620687\n",
            "epoch 285 and loss is: 4.613776683807373 and time: 27.98638617992401\n",
            "epoch 286 and loss is: 4.579344272613525 and time: 28.084774525960288\n",
            "epoch 287 and loss is: 4.574281692504883 and time: 28.183320188522337\n",
            "epoch 288 and loss is: 4.536766052246094 and time: 28.28232609430949\n",
            "epoch 289 and loss is: 4.542908668518066 and time: 28.380679992834725\n",
            "epoch 290 and loss is: 4.52949857711792 and time: 28.47928076585134\n",
            "epoch 291 and loss is: 4.495774745941162 and time: 28.578814566135406\n",
            "epoch 292 and loss is: 4.484238147735596 and time: 28.675895126660667\n",
            "epoch 293 and loss is: 4.490646839141846 and time: 28.774111676216126\n",
            "epoch 294 and loss is: 4.450834274291992 and time: 28.871691222985586\n",
            "epoch 295 and loss is: 4.435292720794678 and time: 28.971933122475942\n",
            "epoch 296 and loss is: 4.424291133880615 and time: 29.070907998085023\n",
            "epoch 297 and loss is: 4.430365085601807 and time: 29.17136691013972\n",
            "epoch 298 and loss is: 4.397452354431152 and time: 29.2707989970843\n",
            "epoch 299 and loss is: 4.382937431335449 and time: 29.372190431753793\n",
            "epoch 300 and loss is: 4.395708084106445 and time: 29.475496927897137\n",
            "epoch 301 and loss is: 4.377143383026123 and time: 29.575171224276225\n",
            "epoch 302 and loss is: 4.34280252456665 and time: 29.68061110575994\n",
            "epoch 303 and loss is: 4.357238292694092 and time: 29.78300424814224\n",
            "epoch 304 and loss is: 4.34834623336792 and time: 29.88180613120397\n",
            "epoch 305 and loss is: 4.34570837020874 and time: 29.979774431387582\n",
            "epoch 306 and loss is: 4.3429274559021 and time: 30.077234864234924\n",
            "epoch 307 and loss is: 4.330432415008545 and time: 30.175456249713896\n",
            "epoch 308 and loss is: 4.301835536956787 and time: 30.275930360953012\n",
            "epoch 309 and loss is: 4.316405296325684 and time: 30.37483495871226\n",
            "epoch 310 and loss is: 4.293856143951416 and time: 30.474347031116487\n",
            "epoch 311 and loss is: 4.29580545425415 and time: 30.574366056919096\n",
            "epoch 312 and loss is: 4.272064208984375 and time: 30.672016259034475\n",
            "epoch 313 and loss is: 4.282320499420166 and time: 30.769745310147602\n",
            "epoch 314 and loss is: 4.262686252593994 and time: 30.867119125525157\n",
            "epoch 315 and loss is: 4.250056743621826 and time: 30.964818660418192\n",
            "epoch 316 and loss is: 4.266390323638916 and time: 31.06215581893921\n",
            "epoch 317 and loss is: 4.24203634262085 and time: 31.160728613535564\n",
            "epoch 318 and loss is: 4.216121196746826 and time: 31.2585867245992\n",
            "epoch 319 and loss is: 4.208555221557617 and time: 31.35646232763926\n",
            "epoch 320 and loss is: 4.210783004760742 and time: 31.454476169745128\n",
            "epoch 321 and loss is: 4.201894283294678 and time: 31.552821346124013\n",
            "epoch 322 and loss is: 4.204136371612549 and time: 31.650786197185518\n",
            "epoch 323 and loss is: 4.206474781036377 and time: 31.749283234278362\n",
            "epoch 324 and loss is: 4.190140247344971 and time: 31.84697828690211\n",
            "epoch 325 and loss is: 4.186864376068115 and time: 31.944349451859793\n",
            "epoch 326 and loss is: 4.224615573883057 and time: 32.04109824895859\n",
            "epoch 327 and loss is: 4.183234691619873 and time: 32.138021465142565\n",
            "epoch 328 and loss is: 4.184579849243164 and time: 32.23477232853572\n",
            "epoch 329 and loss is: 4.182064056396484 and time: 32.33279215097427\n",
            "epoch 330 and loss is: 4.171102046966553 and time: 32.432170085112254\n",
            "epoch 331 and loss is: 4.149277687072754 and time: 32.53397844632467\n",
            "epoch 332 and loss is: 4.171701908111572 and time: 32.63282735745112\n",
            "epoch 333 and loss is: 4.1677470207214355 and time: 32.73030931949616\n",
            "epoch 334 and loss is: 4.16917085647583 and time: 32.82778240442276\n",
            "epoch 335 and loss is: 4.151677131652832 and time: 32.925539485613506\n",
            "epoch 336 and loss is: 4.128987789154053 and time: 33.0233879327774\n",
            "epoch 337 and loss is: 4.133639335632324 and time: 33.12024465799332\n",
            "epoch 338 and loss is: 4.143624305725098 and time: 33.21880264679591\n",
            "epoch 339 and loss is: 4.157087326049805 and time: 33.31606360673904\n",
            "epoch 340 and loss is: 4.128662109375 and time: 33.41362047592799\n",
            "epoch 341 and loss is: 4.127964496612549 and time: 33.5131937901179\n",
            "epoch 342 and loss is: 4.122278690338135 and time: 33.61071681578954\n",
            "epoch 343 and loss is: 4.111937046051025 and time: 33.70811283191045\n",
            "epoch 344 and loss is: 4.129213333129883 and time: 33.80615717172623\n",
            "epoch 345 and loss is: 4.11784553527832 and time: 33.90352379878362\n",
            "epoch 346 and loss is: 4.122727870941162 and time: 34.00183726151784\n",
            "epoch 347 and loss is: 4.113370895385742 and time: 34.10013893842697\n",
            "epoch 348 and loss is: 4.112496376037598 and time: 34.198968509833016\n",
            "epoch 349 and loss is: 4.0917067527771 and time: 34.29689120848973\n",
            "epoch 350 and loss is: 4.113827228546143 and time: 34.39485869407654\n",
            "epoch 351 and loss is: 4.0937089920043945 and time: 34.49714732170105\n",
            "epoch 352 and loss is: 4.1187920570373535 and time: 34.5979865749677\n",
            "epoch 353 and loss is: 4.0838446617126465 and time: 34.695860624313354\n",
            "epoch 354 and loss is: 4.08758020401001 and time: 34.795810250441235\n",
            "epoch 355 and loss is: 4.0896992683410645 and time: 34.897175248463945\n",
            "epoch 356 and loss is: 4.105083465576172 and time: 34.99792052904765\n",
            "epoch 357 and loss is: 4.080881118774414 and time: 35.099101984500884\n",
            "epoch 358 and loss is: 4.1031107902526855 and time: 35.19825892051061\n",
            "epoch 359 and loss is: 4.065657138824463 and time: 35.296438658237456\n",
            "epoch 360 and loss is: 4.084684371948242 and time: 35.39416544834773\n",
            "epoch 361 and loss is: 4.057156085968018 and time: 35.49285899003347\n",
            "epoch 362 and loss is: 4.067841053009033 and time: 35.59130911429723\n",
            "epoch 363 and loss is: 4.048916816711426 and time: 35.68884530464808\n",
            "epoch 364 and loss is: 4.074793338775635 and time: 35.786186544100445\n",
            "epoch 365 and loss is: 4.0467729568481445 and time: 35.88353510697683\n",
            "epoch 366 and loss is: 4.05056619644165 and time: 35.98150202830632\n",
            "epoch 367 and loss is: 4.0830183029174805 and time: 36.07853784958522\n",
            "epoch 368 and loss is: 4.038722038269043 and time: 36.176504890124\n",
            "epoch 369 and loss is: 4.045651435852051 and time: 36.27381970087687\n",
            "epoch 370 and loss is: 4.051159858703613 and time: 36.3713888724645\n",
            "epoch 371 and loss is: 4.0224103927612305 and time: 36.4706557949384\n",
            "epoch 372 and loss is: 4.024167537689209 and time: 36.56859310468038\n",
            "epoch 373 and loss is: 4.018979072570801 and time: 36.666444166501364\n",
            "epoch 374 and loss is: 4.010581970214844 and time: 36.764617296059924\n",
            "epoch 375 and loss is: 4.033249855041504 and time: 36.86260621150335\n",
            "epoch 376 and loss is: 4.055164813995361 and time: 36.96084922154744\n",
            "epoch 377 and loss is: 4.030562877655029 and time: 37.05832815170288\n",
            "epoch 378 and loss is: 4.035452365875244 and time: 37.15720901886622\n",
            "epoch 379 and loss is: 4.001053333282471 and time: 37.25516399542491\n",
            "epoch 380 and loss is: 4.018373489379883 and time: 37.352223165829976\n",
            "epoch 381 and loss is: 4.051049709320068 and time: 37.44970074097316\n",
            "epoch 382 and loss is: 3.993990898132324 and time: 37.548518896102905\n",
            "epoch 383 and loss is: 3.996138334274292 and time: 37.64570636351903\n",
            "epoch 384 and loss is: 4.0281982421875 and time: 37.743473275502524\n",
            "epoch 385 and loss is: 4.021764755249023 and time: 37.841298282146454\n",
            "epoch 386 and loss is: 3.98933482170105 and time: 37.93949658870697\n",
            "epoch 387 and loss is: 4.000274658203125 and time: 38.03780848185222\n",
            "epoch 388 and loss is: 4.017433166503906 and time: 38.13495201269786\n",
            "epoch 389 and loss is: 3.9742343425750732 and time: 38.23316836754481\n",
            "epoch 390 and loss is: 3.9894211292266846 and time: 38.33058341344198\n",
            "epoch 391 and loss is: 3.983440399169922 and time: 38.42854945262273\n",
            "epoch 392 and loss is: 3.978663682937622 and time: 38.52917527357737\n",
            "epoch 393 and loss is: 3.9975028038024902 and time: 38.6274864713351\n",
            "epoch 394 and loss is: 3.9687094688415527 and time: 38.72652515967687\n",
            "epoch 395 and loss is: 3.974390983581543 and time: 38.82510904868444\n",
            "epoch 396 and loss is: 3.971008062362671 and time: 38.92407833735148\n",
            "epoch 397 and loss is: 3.9988467693328857 and time: 39.02219248612722\n",
            "epoch 398 and loss is: 3.967437982559204 and time: 39.119742047786715\n",
            "epoch 399 and loss is: 3.9534316062927246 and time: 39.21937716007233\n",
            "epoch 400 and loss is: 3.9587066173553467 and time: 39.31732335090637\n",
            "epoch 401 and loss is: 3.9595773220062256 and time: 39.41510847806931\n",
            "epoch 402 and loss is: 3.9390993118286133 and time: 39.51472116311391\n",
            "epoch 403 and loss is: 3.957183599472046 and time: 39.61805168390274\n",
            "epoch 404 and loss is: 3.964481830596924 and time: 39.7196036140124\n",
            "epoch 405 and loss is: 3.957367181777954 and time: 39.817429622014366\n",
            "epoch 406 and loss is: 3.9322104454040527 and time: 39.914984146753945\n",
            "epoch 407 and loss is: 3.9389705657958984 and time: 40.01328298648198\n",
            "epoch 408 and loss is: 3.940077781677246 and time: 40.110836486021675\n",
            "epoch 409 and loss is: 3.952538251876831 and time: 40.211638220151265\n",
            "epoch 410 and loss is: 3.9471168518066406 and time: 40.31124505996704\n",
            "epoch 411 and loss is: 3.9236013889312744 and time: 40.41107192436854\n",
            "epoch 412 and loss is: 3.9344277381896973 and time: 40.51273017326991\n",
            "epoch 413 and loss is: 3.939239501953125 and time: 40.61027679045995\n",
            "epoch 414 and loss is: 3.9130070209503174 and time: 40.70921184619268\n",
            "epoch 415 and loss is: 3.933358669281006 and time: 40.807450314362846\n",
            "epoch 416 and loss is: 3.954533338546753 and time: 40.90523035526276\n",
            "epoch 417 and loss is: 3.903878688812256 and time: 41.00421181917191\n",
            "epoch 418 and loss is: 3.9163875579833984 and time: 41.10168609221776\n",
            "epoch 419 and loss is: 3.9181015491485596 and time: 41.20055709282557\n",
            "epoch 420 and loss is: 3.936310052871704 and time: 41.29986936648687\n",
            "epoch 421 and loss is: 3.883716583251953 and time: 41.397221851348874\n",
            "epoch 422 and loss is: 3.917006492614746 and time: 41.49562391042709\n",
            "epoch 423 and loss is: 3.8945066928863525 and time: 41.59385338624318\n",
            "epoch 424 and loss is: 3.9070942401885986 and time: 41.69337340195974\n",
            "epoch 425 and loss is: 3.9113986492156982 and time: 41.793203941981\n",
            "epoch 426 and loss is: 3.917151689529419 and time: 41.89243448177974\n",
            "epoch 427 and loss is: 3.9031736850738525 and time: 41.99510335127513\n",
            "epoch 428 and loss is: 3.901162624359131 and time: 42.099010419845584\n",
            "epoch 429 and loss is: 3.875670909881592 and time: 42.20077855189641\n",
            "epoch 430 and loss is: 3.9122536182403564 and time: 42.302274628480276\n",
            "epoch 431 and loss is: 3.8917784690856934 and time: 42.401111916701\n",
            "epoch 432 and loss is: 3.894047975540161 and time: 42.502523163954415\n",
            "epoch 433 and loss is: 3.9016637802124023 and time: 42.60451790889104\n",
            "epoch 434 and loss is: 3.8895106315612793 and time: 42.70496631066005\n",
            "epoch 435 and loss is: 3.8950870037078857 and time: 42.80372089147568\n",
            "epoch 436 and loss is: 3.8903579711914062 and time: 42.90263927777608\n",
            "epoch 437 and loss is: 3.8752832412719727 and time: 43.00226469834646\n",
            "epoch 438 and loss is: 3.8797781467437744 and time: 43.100161190827684\n",
            "epoch 439 and loss is: 3.8713459968566895 and time: 43.19905070463816\n",
            "epoch 440 and loss is: 3.8684909343719482 and time: 43.298153936862946\n",
            "epoch 441 and loss is: 3.881699562072754 and time: 43.39708975553513\n",
            "epoch 442 and loss is: 3.8665390014648438 and time: 43.49616029659907\n",
            "epoch 443 and loss is: 3.8781418800354004 and time: 43.59530373414358\n",
            "epoch 444 and loss is: 3.8612868785858154 and time: 43.693341040611266\n",
            "epoch 445 and loss is: 3.883662462234497 and time: 43.792595561345415\n",
            "epoch 446 and loss is: 3.850647211074829 and time: 43.89114278157552\n",
            "epoch 447 and loss is: 3.866609811782837 and time: 43.98928277492523\n",
            "epoch 448 and loss is: 3.87674880027771 and time: 44.08704968690872\n",
            "epoch 449 and loss is: 3.8549036979675293 and time: 44.18603312571843\n",
            "Training took 44.25205832719803 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooqFtx7tJnTB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "099565cf-1e86-4528-e86a-2be446c6cbe6"
      },
      "source": [
        "#Plotting our error\n",
        "plt.plot(range(epochs),losses)\n",
        "plt.ylabel('MSE LOSS')\n",
        "plt.xlabel('Epoch') "
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Epoch')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mk1YLKANy1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "980e9b85-62ad-4ec9-b7bf-78fbfaa7e013"
      },
      "source": [
        "#This just turns off the backpropagation, so we can use the model for evaluation rather than training. This helps reduce memory usage and computation speed\n",
        "with torch.no_grad():\n",
        "  y_eval = model.forward(categorical_test, continuous_test)\n",
        "  loss = torch.sqrt(criterion(y_eval, y_test)) #comparing the actual results with the evaluation results\n",
        "\n",
        "print('loss =',loss) #average rmse"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss = tensor(3.8764)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7ZnyCFpOTjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7557f60e-3c69-4878-f6d8-eac5aa9d83b5"
      },
      "source": [
        "for i in range(30):\n",
        "  diff = np.abs(y_eval[i].item() - y_test[i].item())\n",
        "  print(f'{i}.) PREDICTED {y_eval[i].item():8.2f}   TRUE: {y_test[i].item():8.2f}   DIFF: {diff:8.2f}')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.) PREDICTED     7.36   TRUE:     7.70   DIFF:     0.34\n",
            "1.) PREDICTED    16.36   TRUE:    12.90   DIFF:     3.46\n",
            "2.) PREDICTED    13.20   TRUE:    14.90   DIFF:     1.70\n",
            "3.) PREDICTED     3.13   TRUE:     5.70   DIFF:     2.57\n",
            "4.) PREDICTED     9.01   TRUE:     8.50   DIFF:     0.51\n",
            "5.) PREDICTED     6.51   TRUE:    11.70   DIFF:     5.19\n",
            "6.) PREDICTED     6.83   TRUE:     5.30   DIFF:     1.53\n",
            "7.) PREDICTED     5.31   TRUE:     6.50   DIFF:     1.19\n",
            "8.) PREDICTED    11.47   TRUE:    22.10   DIFF:    10.63\n",
            "9.) PREDICTED     7.99   TRUE:     5.30   DIFF:     2.69\n",
            "10.) PREDICTED     3.90   TRUE:     6.90   DIFF:     3.00\n",
            "11.) PREDICTED     7.00   TRUE:     6.50   DIFF:     0.50\n",
            "12.) PREDICTED     9.56   TRUE:     8.10   DIFF:     1.46\n",
            "13.) PREDICTED    25.23   TRUE:    33.87   DIFF:     8.64\n",
            "14.) PREDICTED     4.60   TRUE:     5.70   DIFF:     1.10\n",
            "15.) PREDICTED    26.60   TRUE:    28.50   DIFF:     1.90\n",
            "16.) PREDICTED     8.13   TRUE:    25.07   DIFF:    16.94\n",
            "17.) PREDICTED     8.97   TRUE:    11.30   DIFF:     2.33\n",
            "18.) PREDICTED    15.15   TRUE:    14.90   DIFF:     0.25\n",
            "19.) PREDICTED     6.94   TRUE:     6.50   DIFF:     0.44\n",
            "20.) PREDICTED     7.66   TRUE:    18.10   DIFF:    10.44\n",
            "21.) PREDICTED     8.25   TRUE:    14.10   DIFF:     5.85\n",
            "22.) PREDICTED     4.73   TRUE:     7.30   DIFF:     2.57\n",
            "23.) PREDICTED     8.77   TRUE:    10.90   DIFF:     2.13\n",
            "24.) PREDICTED     7.41   TRUE:     6.50   DIFF:     0.91\n",
            "25.) PREDICTED     5.78   TRUE:     6.50   DIFF:     0.72\n",
            "26.) PREDICTED     4.86   TRUE:     4.90   DIFF:     0.04\n",
            "27.) PREDICTED    12.98   TRUE:    11.30   DIFF:     1.68\n",
            "28.) PREDICTED     3.69   TRUE:     5.30   DIFF:     1.61\n",
            "29.) PREDICTED    15.44   TRUE:    11.30   DIFF:     4.14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNpeeH0HPU0M"
      },
      "source": [
        "torch.save(model.state_dict(), 'taxifare_model.pt') #saving the model"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9OQKsvPkQQf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
